{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "20IyxDzgp3tU"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt # Graphical library\n",
    "#from sklearn.metrics import mean_squared_error # Mean-squared error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Fr69C0UBQk"
   },
   "source": [
    "# Coursework 1 :\n",
    "See pdf for instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QsKvVllvvreH"
   },
   "outputs": [],
   "source": [
    "# WARNING: fill in these two functions that will be used by the auto-marking script\n",
    "# [Action required]\n",
    "\n",
    "def get_CID():\n",
    "  return \"01715447\" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)\n",
    "\n",
    "def get_login():\n",
    "  return \"aa1719\" # Return your short imperial login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKEz3d9NUbdO"
   },
   "source": [
    "## Helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZWnMW3GNpjd7"
   },
   "outputs": [],
   "source": [
    "# This class is used ONLY for graphics\n",
    "# YOU DO NOT NEED to understand it to work on this coursework\n",
    "\n",
    "class GraphicsMaze(object):\n",
    "\n",
    "  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):\n",
    "\n",
    "    self.shape = shape\n",
    "    self.locations = locations\n",
    "    self.absorbing = absorbing\n",
    "\n",
    "    # Walls\n",
    "    self.walls = np.zeros(self.shape)\n",
    "    for ob in obstacle_locs:\n",
    "      self.walls[ob] = 20\n",
    "\n",
    "    # Rewards\n",
    "    self.rewarders = np.ones(self.shape) * default_reward\n",
    "    for i, rew in enumerate(absorbing_locs):\n",
    "      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10\n",
    "\n",
    "    # Print the map to show it\n",
    "    self.paint_maps()\n",
    "\n",
    "  def paint_maps(self):\n",
    "    \"\"\"\n",
    "    Print the Maze topology (obstacles, absorbing states and rewards)\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders)\n",
    "    plt.show()\n",
    "\n",
    "  def paint_state(self, state):\n",
    "    \"\"\"\n",
    "    Print one state on the Maze topology (obstacles, absorbing states and rewards)\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    states = np.zeros(self.shape)\n",
    "    states[state] = 30\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders + states)\n",
    "    plt.show()\n",
    "\n",
    "  def draw_deterministic_policy(self, Policy):\n",
    "    \"\"\"\n",
    "    Draw a deterministic policy\n",
    "    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
    "    for state, action in enumerate(Policy):\n",
    "      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
    "        continue\n",
    "      arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
    "      action_arrow = arrows[action] # Take the corresponding action\n",
    "      location = self.locations[state] # Compute its location on graph\n",
    "      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
    "    plt.show()\n",
    "\n",
    "  def draw_policy(self, Policy):\n",
    "    \"\"\"\n",
    "    Draw a policy (draw an arrow in the most probable direction)\n",
    "    input: Policy {np.array} -- policy to draw as probability\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])\n",
    "    self.draw_deterministic_policy(deterministic_policy)\n",
    "\n",
    "  def draw_value(self, Value):\n",
    "    \"\"\"\n",
    "    Draw a policy value\n",
    "    input: Value {np.array} -- policy values to draw\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
    "    for state, value in enumerate(Value):\n",
    "      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value\n",
    "        continue\n",
    "      location = self.locations[state] # Compute the value location on graph\n",
    "      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
    "    plt.show()\n",
    "\n",
    "  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):\n",
    "    \"\"\"\n",
    "    Draw a grid representing multiple deterministic policies\n",
    "    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for subplot in range (len(Policies)): # Go through all policies\n",
    "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy\n",
    "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
    "      for state, action in enumerate(Policies[subplot]):\n",
    "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
    "          continue\n",
    "        arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
    "        action_arrow = arrows[action] # Take the corresponding action\n",
    "        location = self.locations[state] # Compute its location on graph\n",
    "        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
    "      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument\n",
    "    plt.show()\n",
    "\n",
    "  def draw_policy_grid(self, Policies, title, n_columns, n_lines):\n",
    "    \"\"\"\n",
    "    Draw a grid representing multiple policies (draw an arrow in the most probable direction)\n",
    "    input: Policy {np.array} -- array of policies to draw as probability\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])\n",
    "    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)\n",
    "\n",
    "  def draw_value_grid(self, Values, title, n_columns, n_lines):\n",
    "    \"\"\"\n",
    "    Draw a grid representing multiple policy values\n",
    "    input: Values {np.array of np.array} -- array of policy values to draw\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for subplot in range (len(Values)): # Go through all values\n",
    "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value\n",
    "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
    "      for state, value in enumerate(Values[subplot]):\n",
    "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
    "          continue\n",
    "        location = self.locations[state] # Compute the value location on graph\n",
    "        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph\n",
    "      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbY8DCqoVJlw"
   },
   "source": [
    "## Maze class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MXc1OFvZqJfZ"
   },
   "outputs": [],
   "source": [
    "# This class define the Maze environment\n",
    "\n",
    "class Maze(object):\n",
    "\n",
    "  # [Action required]\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Maze initialisation.\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "    \n",
    "    # [Action required]\n",
    "    # Properties set from the CID\n",
    "    cid = \"01715447\" # so it's self-contained / get_CID()\n",
    "    cid_second_to_last_digit =  int(cid[-2]) # In Docs this is y\n",
    "    cid_last_digit = int(cid[-1]) # In docs this is z\n",
    "\n",
    "    self._prob_success = 0.8 + 0.02 * (9 - cid_second_to_last_digit) # float 0.8 + 0.02 * (9 - y)\n",
    "    self._gamma = 0.8 + 0.02 * cid_second_to_last_digit # float 0.8 + 0.02y\n",
    "    self._goal = cid_last_digit % 4 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3) z mod 4\n",
    "\n",
    "    # Build the maze\n",
    "    self._build_maze()\n",
    "                              \n",
    "\n",
    "  # Functions used to build the Maze environment \n",
    "  # You DO NOT NEED to modify them\n",
    "  def _build_maze(self):\n",
    "    \"\"\"\n",
    "    Maze initialisation.\n",
    "    input: /\n",
    "    output: /\n",
    "    \"\"\"\n",
    "\n",
    "    # Properties of the maze\n",
    "    self._shape = (13, 10)\n",
    "    self._obstacle_locs = [\n",
    "                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \\\n",
    "                          (2,1), (2,2), (2,3), (2,7), \\\n",
    "                          (3,1), (3,2), (3,3), (3,7), \\\n",
    "                          (4,1), (4,7), \\\n",
    "                          (5,1), (5,7), \\\n",
    "                          (6,5), (6,6), (6,7), \\\n",
    "                          (8,0), \\\n",
    "                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \\\n",
    "                          (10,0)\n",
    "                         ] # Location of obstacles\n",
    "    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states\n",
    "    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]\n",
    "    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states\n",
    "    self._default_reward = -1 # Reward for each action performs in the environment\n",
    "    self._max_t = 500 # Max number of steps in the environment\n",
    "\n",
    "    # Actions\n",
    "    self._action_size = 4\n",
    "    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on\n",
    "        \n",
    "    # States\n",
    "    self._locations = []\n",
    "    for i in range (self._shape[0]):\n",
    "      for j in range (self._shape[1]):\n",
    "        loc = (i,j) \n",
    "        # Adding the state to locations if it is no obstacle\n",
    "        if self._is_location(loc):\n",
    "          self._locations.append(loc)\n",
    "    self._state_size = len(self._locations)\n",
    "\n",
    "    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)\n",
    "    self._neighbours = np.zeros((self._state_size, 4)) \n",
    "    \n",
    "    for state in range(self._state_size):\n",
    "      loc = self._get_loc_from_state(state)\n",
    "\n",
    "      # North\n",
    "      neighbour = (loc[0]-1, loc[1]) # North neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('N')] = state\n",
    "\n",
    "      # East\n",
    "      neighbour = (loc[0], loc[1]+1) # East neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('E')] = state\n",
    "\n",
    "      # South\n",
    "      neighbour = (loc[0]+1, loc[1]) # South neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('S')] = state\n",
    "\n",
    "      # West\n",
    "      neighbour = (loc[0], loc[1]-1) # West neighbours location\n",
    "      if self._is_location(neighbour):\n",
    "        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)\n",
    "      else: # If there is no neighbour in this direction, coming back to current state\n",
    "        self._neighbours[state][self._direction_names.index('W')] = state\n",
    "\n",
    "    # Absorbing\n",
    "    self._absorbing = np.zeros((1, self._state_size))\n",
    "    for a in self._absorbing_locs:\n",
    "      absorbing_state = self._get_state_from_loc(a)\n",
    "      self._absorbing[0, absorbing_state] = 1\n",
    "\n",
    "    # Transition matrix\n",
    "    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A\n",
    "    for action in range(self._action_size):\n",
    "      for outcome in range(4): # For each direction (N, E, S, W)\n",
    "        # The agent has prob_success probability to go in the correct direction\n",
    "        if action == outcome:\n",
    "          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)\n",
    "        # Equal probability to go into one of the other directions\n",
    "        else:\n",
    "          prob = (1.0 - self._prob_success) / 3.0\n",
    "          \n",
    "        # Write this probability in the transition matrix\n",
    "        for prior_state in range(self._state_size):\n",
    "          # If absorbing state, probability of 0 to go to any other states\n",
    "          if not self._absorbing[0, prior_state]:\n",
    "            post_state = self._neighbours[prior_state, outcome] # Post state number\n",
    "            post_state = int(post_state) # Transform in integer to avoid error\n",
    "            self._T[prior_state, post_state, action] += prob\n",
    "\n",
    "    # Reward matrix\n",
    "    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1\n",
    "    self._R = self._default_reward * self._R # Set default_reward everywhere\n",
    "    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards\n",
    "      post_state = self._get_state_from_loc(self._absorbing_locs[i])\n",
    "      self._R[:,post_state,:] = self._absorbing_rewards[i]\n",
    "\n",
    "    # Creating the graphical Maze world\n",
    "    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)\n",
    "    \n",
    "    # Reset the environment\n",
    "    self.reset()\n",
    "\n",
    "\n",
    "  def _is_location(self, loc):\n",
    "    \"\"\"\n",
    "    Is the location a valid state (not out of Maze and not an obstacle)\n",
    "    input: loc {tuple} -- location of the state\n",
    "    output: _ {bool} -- is the location a valid state\n",
    "    \"\"\"\n",
    "    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):\n",
    "      return False\n",
    "    elif (loc in self._obstacle_locs):\n",
    "      return False\n",
    "    else:\n",
    "      return True\n",
    "\n",
    "\n",
    "  def _get_state_from_loc(self, loc):\n",
    "    \"\"\"\n",
    "    Get the state number corresponding to a given location\n",
    "    input: loc {tuple} -- location of the state\n",
    "    output: index {int} -- corresponding state number\n",
    "    \"\"\"\n",
    "    return self._locations.index(tuple(loc))\n",
    "\n",
    "\n",
    "  def _get_loc_from_state(self, state):\n",
    "    \"\"\"\n",
    "    Get the state number corresponding to a given location\n",
    "    input: index {int} -- state number\n",
    "    output: loc {tuple} -- corresponding location\n",
    "    \"\"\"\n",
    "    return self._locations[state]\n",
    "\n",
    "  # Getter functions used only for DP agents\n",
    "  # You DO NOT NEED to modify them\n",
    "  def get_T(self):\n",
    "    return self._T\n",
    "\n",
    "  def get_R(self):\n",
    "    return self._R\n",
    "\n",
    "  def get_absorbing(self):\n",
    "    return self._absorbing\n",
    "\n",
    "  # Getter functions used for DP, MC and TD agents\n",
    "  # You DO NOT NEED to modify them\n",
    "  def get_graphics(self):\n",
    "    return self._graphics\n",
    "\n",
    "  def get_action_size(self):\n",
    "    return self._action_size\n",
    "\n",
    "  def get_state_size(self):\n",
    "    return self._state_size\n",
    "\n",
    "  def get_gamma(self):\n",
    "    return self._gamma\n",
    "\n",
    "  # Functions used to perform episodes in the Maze environment\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment state to one of the possible starting states\n",
    "    input: /\n",
    "    output: \n",
    "      - t {int} -- current timestep\n",
    "      - state {int} -- current state of the envionment\n",
    "      - reward {int} -- current reward\n",
    "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
    "    \"\"\"\n",
    "    self._t = 0\n",
    "    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])\n",
    "    self._reward = 0\n",
    "    self._done = False\n",
    "    return self._t, self._state, self._reward, self._done\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    Perform an action in the environment\n",
    "    input: action {int} -- action to perform\n",
    "    output: \n",
    "      - t {int} -- current timestep\n",
    "      - state {int} -- current state of the envionment\n",
    "      - reward {int} -- current reward\n",
    "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # If environment already finished, print an error\n",
    "    if self._done or self._absorbing[0, self._state]:\n",
    "      print(\"Please reset the environment\")\n",
    "      return self._t, self._state, self._reward, self._done\n",
    "\n",
    "    # Drawing a random number used for probaility of next state\n",
    "    probability_success = random.uniform(0,1)\n",
    "\n",
    "    # Look for the first possible next states (so get a reachable state even if probability_success = 0)\n",
    "    new_state = 0\n",
    "    while self._T[self._state, new_state, action] == 0: \n",
    "      new_state += 1\n",
    "    assert self._T[self._state, new_state, action] != 0, \"Selected initial state should be probability 0, something might be wrong in the environment.\"\n",
    "\n",
    "    # Find the first state for which probability of occurence matches the random value\n",
    "    total_probability = self._T[self._state, new_state, action]\n",
    "    while (total_probability < probability_success) and (new_state < self._state_size-1):\n",
    "     new_state += 1\n",
    "     total_probability += self._T[self._state, new_state, action]\n",
    "    assert self._T[self._state, new_state, action] != 0, \"Selected state should be probability 0, something might be wrong in the environment.\"\n",
    "    \n",
    "    # Setting new t, state, reward and done\n",
    "    self._t += 1\n",
    "    self._reward = self._R[self._state, new_state, action]\n",
    "    self._done = self._absorbing[0, new_state] or self._t > self._max_t\n",
    "    self._state = new_state\n",
    "    return self._t, self._state, self._reward, self._done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def visualize_value_function(maze_env, V, filename):\n",
    "    n, m = maze_env._shape\n",
    "    value_grid = np.full(maze_env._shape, np.nan)  # Create a grid full of NaNs\n",
    "\n",
    "    # Map the value function V onto our grid\n",
    "    for state in range(maze_env.get_state_size()):\n",
    "        i, j = maze_env._get_loc_from_state(state)\n",
    "        value_grid[i][j] = V[state]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(m, n))\n",
    "\n",
    "    # Create the heatmap\n",
    "    cax = ax.matshow(value_grid, cmap='viridis')  # 'viridis' is a colormap suitable for heatmaps\n",
    "\n",
    "    # Mark obstacles and absorbing states\n",
    "    for i, j in maze_env._obstacle_locs:\n",
    "        ax.text(j, i, '█', ha='center', va='center', fontsize=12, color='white')\n",
    "    for i, j in maze_env._absorbing_locs:\n",
    "        ax.text(j, i, 'A', ha='center', va='center', fontsize=12, color='white')\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticks(np.arange(-0.5, m, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(which='both')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "def visualize_stochastic_policy(maze_env, policy, filename):\n",
    "    n, m = maze_env._shape\n",
    "    fig, ax = plt.subplots(figsize=(m, n))\n",
    "\n",
    "    # Define a function to draw arrows based on action and probability\n",
    "    def draw_arrow(x, y, action, probability):\n",
    "        if action == 0:   # North\n",
    "            ax.arrow(x, y, 0, -0.3, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n",
    "        elif action == 1: # East\n",
    "            ax.arrow(x, y, 0.3, 0, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n",
    "        elif action == 2: # South\n",
    "            ax.arrow(x, y, 0, 0.3, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n",
    "        elif action == 3: # West\n",
    "            ax.arrow(x, y, -0.3, 0, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n",
    "\n",
    "    # Loop over states and draw arrows based on the policy probabilities\n",
    "    for state in range(maze_env.get_state_size()):\n",
    "        i, j = maze_env._get_loc_from_state(state)\n",
    "        if (i, j) not in maze_env._obstacle_locs:\n",
    "            for action, prob in enumerate(policy[state]):\n",
    "                if prob > 0:  # Only draw if there's a non-zero probability\n",
    "                    draw_arrow(j, i, action, prob)\n",
    "\n",
    "    # Mark obstacles and absorbing states\n",
    "    for i, j in maze_env._obstacle_locs:\n",
    "        ax.text(j, i, '█', ha='center', va='center', fontsize=12, color='red')\n",
    "    for i, j in maze_env._absorbing_locs:\n",
    "        ax.text(j, i, 'A', ha='center', va='center', fontsize=12, color='blue')\n",
    "\n",
    "    ax.set_xlim(-0.5, m-0.5)\n",
    "    ax.set_ylim(-0.5, n-0.5)\n",
    "    ax.set_xticks(np.arange(-0.5, m, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(which='both')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "def visualize_policy(maze_env, policy, filename):\n",
    "    n, m = maze_env._shape\n",
    "    fig, ax = plt.subplots(figsize=(m, n))\n",
    "\n",
    "    # Directions for arrows based on action index\n",
    "    directions = np.array([[0, -1], [-1, 0], [0, 1], [1, 0]])\n",
    "    arrow_directions = ['↑', '→', '↓', '←']\n",
    "\n",
    "    for state in range(maze_env.get_state_size()):\n",
    "        i, j = maze_env._get_loc_from_state(state)\n",
    "\n",
    "        # Get the action with the highest probability for the state\n",
    "        best_action = np.argmax(policy[state])\n",
    "        arrow = arrow_directions[best_action]\n",
    "\n",
    "        ax.text(j, i, arrow, ha='center', va='center', fontsize=12)\n",
    "\n",
    "    # Mark obstacles and absorbing states\n",
    "    for i, j in maze_env._obstacle_locs:\n",
    "        ax.text(j, i, '█', ha='center', va='center', fontsize=12, color='red')\n",
    "    for i, j in maze_env._absorbing_locs:\n",
    "        ax.text(j, i, 'A', ha='center', va='center', fontsize=12, color='green')\n",
    "\n",
    "    ax.set_xticks(np.arange(-0.5, m, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(which='both')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_value_and_hyperparameters(maze_env, V, hyperparams, filename):\n",
    "    # Extract hyperparameters for the bar chart\n",
    "    [policy_improvement, batch_limit, episodes, visit_type, exploratory_decay, exploratory_param ]= hyperparams\n",
    "\n",
    "    # First, visualize the value function\n",
    "    n, m = maze_env._shape\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(m*2, n))  # 1 row, 2 columns\n",
    "\n",
    "    # --- Value function visualization (as you provided) ---\n",
    "    value_grid = np.full(maze_env._shape, np.nan)\n",
    "    for state in range(maze_env.get_state_size()):\n",
    "        i, j = maze_env._get_loc_from_state(state)\n",
    "        value_grid[i][j] = V[state]\n",
    "    cax1 = ax1.matshow(value_grid, cmap='viridis')\n",
    "    fig.colorbar(cax1, ax=ax1, orientation='vertical')\n",
    "\n",
    "    # --- Hyperparameters Bar Chart ---\n",
    "    labels = ['Batch Limit', 'Episodes', 'Visit Type', 'Exploration Decay', 'Exploration Param']\n",
    "    normalized_values = [\n",
    "        batch_limit / max(MC_Batch_Limit),\n",
    "        episodes / max(MC_Episodes_in_Batch),\n",
    "        1 if visit_type else 0,\n",
    "        (exploratory_decay - min(MC_Exploration_Decay)) / (max(MC_Exploration_Decay) - min(MC_Exploration_Decay)),\n",
    "        (exploratory_param - min(MC_Exploration_Params)) / (max(MC_Exploration_Params) - min(MC_Exploration_Params))\n",
    "    ]\n",
    "\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    bars = ax2.barh(labels, normalized_values, color=colors)\n",
    "\n",
    "    bars = ax2.barh(labels, normalized_values, color=colors)\n",
    "    ax2.set_title(policy_improvement)\n",
    "    ax2.set_xlim(0, 1)  # As values are normalized\n",
    "\n",
    "    # Display the actual values on top of each bar\n",
    "    for bar, value, label in zip(bars, [batch_limit, episodes, visit_type, exploratory_decay, exploratory_param], labels):\n",
    "        ax2.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2, str(value),\n",
    "                 va='center', ha='left', color='black', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "def visualize_value_policy_and_hyperparameters(maze_env, V, policy, hyperparams, filename):\n",
    "    # Extract hyperparameters for the bar chart\n",
    "    [policy_improvement, batch_limit, episodes, visit_type, exploratory_decay, exploratory_param] = hyperparams\n",
    "\n",
    "    # First, visualize the value function and policy\n",
    "    n, m = maze_env._shape\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(m*3, n))  # 1 row, 3 columns\n",
    "\n",
    "    # --- Value function visualization ---\n",
    "    value_grid = np.full(maze_env._shape, np.nan)\n",
    "    for state in range(maze_env.get_state_size()):\n",
    "        i, j = maze_env._get_loc_from_state(state)\n",
    "        value_grid[i][j] = V[state]\n",
    "    cax1 = ax1.matshow(value_grid, cmap='viridis')\n",
    "    fig.colorbar(cax1, ax=ax1, orientation='vertical')\n",
    "\n",
    "    # --- Policy visualization ---\n",
    "    for state in range(maze_env.get_state_size()):\n",
    "        i, j = maze_env._get_loc_from_state(state)\n",
    "        if (i, j) not in maze_env._obstacle_locs:\n",
    "            for action, prob in enumerate(policy[state]):\n",
    "                if prob > 0:  # Only draw if there's a non-zero probability\n",
    "                    draw_arrow(ax2, j, i, action, prob)\n",
    "    for i, j in maze_env._obstacle_locs:\n",
    "        ax2.text(j, i, '█', ha='center', va='center', fontsize=12, color='red')\n",
    "    for i, j in maze_env._absorbing_locs:\n",
    "        ax2.text(j, i, 'A', ha='center', va='center', fontsize=12, color='blue')\n",
    "    ax2.set_xlim(-0.5, m-0.5)\n",
    "    ax2.set_ylim(-0.5, n-0.5)\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    # --- Hyperparameters Bar Chart ---\n",
    "    labels = ['Batch Limit', 'Episodes', 'Visit Type', 'Exploration Decay', 'Exploration Param']\n",
    "    normalized_values = [\n",
    "        batch_limit / max(MC_Batch_Limit),\n",
    "        episodes / max(MC_Episodes_in_Batch),\n",
    "        1 if visit_type else 0,\n",
    "        (exploratory_decay - min(MC_Exploration_Decay)) / (max(MC_Exploration_Decay) - min(MC_Exploration_Decay)),\n",
    "        (exploratory_param - min(MC_Exploration_Params)) / (max(MC_Exploration_Params) - min(MC_Exploration_Params))\n",
    "    ]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    bars = ax3.barh(labels, normalized_values, color=colors)\n",
    "    ax3.set_title(policy_improvement)\n",
    "    ax3.set_xlim(0, 1)  # As values are normalized\n",
    "    for bar, value, label in zip(bars, [batch_limit, episodes, visit_type, exploratory_decay, exploratory_param], labels):\n",
    "        ax3.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2, str(value),\n",
    "                 va='center', ha='left', color='black', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "def draw_arrow(ax, x, y, action, probability):\n",
    "    if action == 0:   # North\n",
    "        ax.arrow(x, y, 0, -0.3, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n",
    "    elif action == 1: # East\n",
    "        ax.arrow(x, y, 0.3, 0, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n",
    "    elif action == 2: # South\n",
    "        ax.arrow(x, y, 0, 0.3, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n",
    "    elif action == 3: # West\n",
    "        ax.arrow(x, y, -0.3, 0, head_width=0.2, head_length=0.2, fc='k', ec='k', alpha=probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW3Ul0q-VRE-"
   },
   "source": [
    "## DP Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3ucYXx5NqStY"
   },
   "outputs": [],
   "source": [
    "# This class define the Dynamic Programing agent \n",
    "\n",
    "class DP_agent(object):\n",
    "\n",
    "  # [Action required]\n",
    "  # WARNING: make sure this function can be called by the auto-marking script\n",
    "  def solve(self, env):\n",
    "    \"\"\"\n",
    "    Solve a given Maze environment using Dynamic Programming\n",
    "    input: env {Maze object} -- Maze to solve\n",
    "    output: \n",
    "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
    "      - V {np.array} -- Corresponding value function \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisation (can be edited)\n",
    "    policy = np.zeros((env.get_state_size(), env.get_action_size())) \n",
    "    V = np.zeros(env.get_state_size())\n",
    "\n",
    "    #### \n",
    "    # Add your code here\n",
    "    # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()\n",
    "    ####\n",
    "    transition_matrix = env.get_T()\n",
    "    reward_matrix = env.get_R()\n",
    "    absorbing = env.get_absorbing()\n",
    "    discount_factor = env.get_gamma()\n",
    "\n",
    "    RECORD_PROGRESS = False\n",
    "    is_policy_stable = False\n",
    "    iteration_steps = 1\n",
    "\n",
    "    while not is_policy_stable:\n",
    "      # print(\"iteration step: \", iteration_steps)\n",
    "      if RECORD_PROGRESS:\n",
    "        visualize_value_function(env, V, filename=f\"./images/DP/ValueFunction/V-Value-Iteration-{iteration_steps}.png\")\n",
    "        visualize_stochastic_policy(env, policy, filename=f\"./images/DP/Policy/Policy-Iteration-{iteration_steps}.png\")\n",
    "\n",
    "      policy_iteration = self.iterate_policy(\n",
    "        policy=policy,\n",
    "        transition_matrix=transition_matrix,\n",
    "        reward_matrix=reward_matrix,\n",
    "        discount_factor=discount_factor,\n",
    "        V=V,\n",
    "        states=range(env.get_state_size()),\n",
    "        actions=range(env.get_action_size())\n",
    "      )\n",
    "\n",
    "      V_iteration = self.evaluate_policy(\n",
    "        transition_matrix=transition_matrix,\n",
    "        reward_matrix=reward_matrix,\n",
    "        discount_factor=discount_factor,\n",
    "        V=V,\n",
    "        policy=policy_iteration,\n",
    "        states=range(env.get_state_size()),\n",
    "        actions=range(env.get_action_size())\n",
    "      )\n",
    "\n",
    "      mean_absolute_difference = np.mean(np.abs(V - V_iteration))\n",
    "      max_absolute_difference = np.max(np.abs(V - V_iteration))\n",
    "\n",
    "      print(\"Policy Mean Absolute Difference:\", mean_absolute_difference)\n",
    "      print(\"Policy Max Absolute Difference:\", max_absolute_difference)\n",
    "\n",
    "      # print(policy, policy_iteration)\n",
    "      if max_absolute_difference <= 1e-10:\n",
    "        is_policy_stable = True\n",
    "\n",
    "      policy, V = policy_iteration, V_iteration\n",
    "      iteration_steps += 1\n",
    "\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "  def evaluate_policy(self, transition_matrix, reward_matrix, discount_factor, V, policy, states, actions):\n",
    "    policy_eval = np.copy(V)\n",
    "    for s in states:\n",
    "      state_value = 0\n",
    "      for action in actions:\n",
    "        policy_action = policy[s][action]\n",
    "\n",
    "        # Probability\n",
    "        s_to_s_prime_action_reward = 0\n",
    "        for s_prime in states:\n",
    "          s_to_s_prime_action_reward += transition_matrix[s][s_prime][action] * (\n",
    "            reward_matrix[s][s_prime][action] + discount_factor * V[s_prime]\n",
    "          )\n",
    "\n",
    "        state_value += policy_action * s_to_s_prime_action_reward\n",
    "      policy_eval[s] = state_value\n",
    "    return policy_eval\n",
    "\n",
    "  def soft_max(self, x):\n",
    "    # Use this to get a stocastic policy\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "  def iterate_policy(self, policy, transition_matrix, reward_matrix, discount_factor, V, states, actions):\n",
    "    new_policy = np.copy(policy)\n",
    "\n",
    "    for s in states:\n",
    "      action_values = np.zeros(len(actions))\n",
    "      for a in actions:\n",
    "        a_value = 0\n",
    "        for s_prime in states:\n",
    "          s_a_s_prime_value = transition_matrix[s][s_prime][a] * (\n",
    "            reward_matrix[s][s_prime][a] + discount_factor * (V[s_prime])\n",
    "          )\n",
    "          a_value += s_a_s_prime_value\n",
    "\n",
    "        action_values[a] = a_value\n",
    "\n",
    "      new_policy[s] = self.soft_max(action_values)\n",
    "\n",
    "    return new_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14i0zRkdVSqk"
   },
   "source": [
    "## MC agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CdHvHvcSrEW9"
   },
   "outputs": [],
   "source": [
    "# This class define the Monte-Carlo agent\n",
    "\n",
    "class MC_agent(object):\n",
    "  \n",
    "  # [Action required]\n",
    "  # WARNING: make sure this function can be called by the auto-marking script\n",
    "  def solve(self, env, FIRST_VISIT_MC, episodes_in_batch, exploration_param, exploration_decay, batch_limit, policy_improvement_type):\n",
    "    \"\"\"\n",
    "    Solve a given Maze environment using Monte Carlo learning\n",
    "    input: env {Maze object} -- Maze to solve\n",
    "    output: \n",
    "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
    "      - values {list of np.array} -- List of successive value functions for each episode \n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
    "    \"\"\"\n",
    "\n",
    "    # Options\n",
    "    FIRST_VISIT_MC = FIRST_VISIT_MC\n",
    "    episodes_in_batch = episodes_in_batch\n",
    "    exploration_param = exploration_param # epsilon\n",
    "    exploration_decay = exploration_decay # Set to 1 for zero decay\n",
    "    batch_limit = batch_limit\n",
    "    policy_improvement_type = policy_improvement_type # E-GREEDY = epsilon greey, SOFT-MAX = soft max, COMBINATION = both\n",
    "\n",
    "    # Initialisation (can be edited)\n",
    "    Q = np.random.rand(env.get_state_size(), env.get_action_size()) \n",
    "    V = np.zeros(env.get_state_size())\n",
    "    num_actions = env.get_action_size()\n",
    "    uniform_probability = 1.0 / num_actions\n",
    "    policy = np.full((env.get_state_size(), num_actions), uniform_probability)\n",
    "    values = []\n",
    "    total_rewards = []\n",
    "\n",
    "    #### \n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    batch = 0\n",
    "    batches = []\n",
    "\n",
    "    while batch <= batch_limit:\n",
    "      # print(f\"Beginning Batch, {batch}\")\n",
    "      # Let's do it for a single run through\n",
    "      episodes = []\n",
    "\n",
    "      # Running the Policy\n",
    "      for episode_number in range(episodes_in_batch):\n",
    "        # print(f\"Completing Episode: {episode_number}\")\n",
    "        current_timestep, current_state, current_reward, is_finished = env.reset()\n",
    "        action_picked = self.select_action_from_stocastic_policy(policy[current_state])\n",
    "        episode = [(current_timestep, current_state, action_picked, current_reward, is_finished)]\n",
    "\n",
    "        while not is_finished:\n",
    "            current_timestep, current_state, current_reward, is_finished = env.step(action_picked)\n",
    "            action_picked = self.select_action_from_stocastic_policy(policy[current_state])\n",
    "            episode.append((current_timestep, current_state, action_picked, current_reward, is_finished))\n",
    "\n",
    "        _value = self.get_value_of_episode(env, episode)\n",
    "        _total_rewards = sum([i[3] for i in episode])\n",
    "        values.append(_value)\n",
    "        # print(\"Total Rewards: \", _total_rewards)\n",
    "        total_rewards.append(_total_rewards)\n",
    "        episodes.append(episode)\n",
    "\n",
    "      batches.append(episodes)\n",
    "\n",
    "      # First Visit Policy Evaluation\n",
    "      new_Q = self.evaluate_episodes(env, episodes, first_visit=FIRST_VISIT_MC)\n",
    "\n",
    "      mean_absolute_difference = np.mean(np.abs(Q - new_Q))\n",
    "      max_absolute_difference = np.max(np.abs(Q - new_Q))\n",
    "\n",
    "      #print(\"Policy Mean Absolute Difference:\", mean_absolute_difference)\n",
    "      # print(\"Policy Max Absolute Difference:\", max_absolute_difference)\n",
    "\n",
    "      Q= new_Q\n",
    "\n",
    "      old_policy = np.copy(policy)\n",
    "\n",
    "      # Policy Improvement\n",
    "      for s in range(env.get_state_size()):\n",
    "        # Get softmax probabilities for actions\n",
    "        soft_probs = self.soft_max(Q[s])\n",
    "        best_action = np.argmax(Q[s])\n",
    "\n",
    "        # Combine ε-greedy with softmax\n",
    "        for a in range(env.get_action_size()):\n",
    "            if policy_improvement_type == \"E-GREEDY\":\n",
    "                if a == best_action:\n",
    "                    policy[s][a] = 1 - exploration_param + (exploration_param / env.get_action_size())\n",
    "                else:\n",
    "                    policy[s][a] = exploration_param / env.get_action_size()\n",
    "\n",
    "            if policy_improvement_type == \"COMBINATION\":\n",
    "                policy[s][a] = exploration_param / env.get_action_size() + (1 - exploration_param) * soft_probs[a]\n",
    "\n",
    "            if policy_improvement_type == \"SOFT-MAX\":\n",
    "                policy[s][a] = soft_probs[a]\n",
    "\n",
    "      batch += 1\n",
    "      exploration_param *= exploration_decay\n",
    "\n",
    "    return policy, values, total_rewards, batches\n",
    "\n",
    "  def soft_max(self, x):\n",
    "    # Use this to get a stocastic policy\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "  def get_value_of_episode(self, env, episode):\n",
    "    _rewards = [0 for s in range(env.get_state_size())]\n",
    "    _visited = [False for s in range(env.get_state_size())]\n",
    "    for index, episode_timestep in enumerate(episode):\n",
    "        # Value of being in the current state\n",
    "        (current_timestep, current_state, action_picked, current_reward, _) = episode_timestep\n",
    "\n",
    "        if _visited[current_state]:\n",
    "            continue\n",
    "\n",
    "        value_of_state = sum([elem[3]*(env.get_gamma()**i) for i, elem in enumerate(episode[index:])])\n",
    "        _visited[current_state] = True\n",
    "        _rewards[current_state] = value_of_state\n",
    "\n",
    "    return _rewards\n",
    "\n",
    "  def evaluate_episodes_first_visit(self, env, episodes):\n",
    "    _q = [[0 for a in range(env.get_action_size())] for s in range(env.get_state_size())]\n",
    "    _returns = [[[] for a in range(env.get_action_size())] for s in range(env.get_state_size())]\n",
    "\n",
    "    for episode in episodes:\n",
    "      # To make sure you're not visiting a state more than once.\n",
    "      _visited = set()\n",
    "      for index, episode_timestep in enumerate(episode):\n",
    "        (current_timestep, current_state, action_picked, current_reward, _) = episode_timestep\n",
    "\n",
    "        if (current_state, action_picked) in _visited:\n",
    "            continue\n",
    "        _visited.add((current_state, action_picked))\n",
    "        value_of_state = sum([elem[3]*(env.get_gamma()**i) for i, elem in enumerate(episode[index:])])\n",
    "        _returns[current_state][action_picked].append(value_of_state)\n",
    "        _q[current_state][action_picked] = np.mean(_returns[current_state][action_picked])\n",
    "\n",
    "    return np.array(_q)\n",
    "\n",
    "  def evaluate_episodes_every_visit(self, env, episodes):\n",
    "    _q = [[0 for a in range(env.get_action_size())] for s in range(env.get_state_size())]\n",
    "    _returns = [[[] for a in range(env.get_action_size())] for s in range(env.get_state_size())]\n",
    "\n",
    "    for episode in episodes:\n",
    "      for index, episode_timestep in enumerate(episode):\n",
    "        (current_timestep, current_state, action_picked, current_reward, _) = episode_timestep\n",
    "\n",
    "        value_of_state = sum([elem[3]*(env.get_gamma()**i) for i, elem in enumerate(episode[index:])])\n",
    "        _returns[current_state][action_picked].append(value_of_state)\n",
    "        _q[current_state][action_picked] = np.mean(_returns[current_state][action_picked])\n",
    "\n",
    "    return np.array(_q)\n",
    "\n",
    "  def evaluate_episodes(self, env, episodes, first_visit=True):\n",
    "    if first_visit:\n",
    "        return self.evaluate_episodes_first_visit(env, episodes)\n",
    "    else:\n",
    "        return self.evaluate_episodes_every_visit(env, episodes)\n",
    "\n",
    "  def select_action_from_stocastic_policy(self, action_distribution):\n",
    "    sum_of_distribution = sum(action_distribution)\n",
    "    ERROR_BOUND = 0.005\n",
    "    if sum_of_distribution <= 1 - ERROR_BOUND or sum_of_distribution >= 1 + ERROR_BOUND:\n",
    "      print(f\"Sum of action_distribution: {sum_of_distribution}, {int(sum_of_distribution)}\")\n",
    "      assert False, \"Policy distribution sum is not equal to 1\"  # Raising an assertion with a custom message\n",
    "\n",
    "    probability = random.random()\n",
    "    current_total = 0\n",
    "    for index, action_probability in enumerate(action_distribution):\n",
    "          current_total += action_probability\n",
    "          if probability <= current_total:\n",
    "              return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Maze:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x1000 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 1.0, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.95, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.9, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.85, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.8, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.75, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.7, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.65, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.6, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:1, decay: 0.55, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 1.0, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.95, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.9, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.85, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.8, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.75, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.7, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.65, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.6, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:1, decay: 0.55, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 1.0, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.95, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.9, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.85, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.8, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.75, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.7, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.65, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.6, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:2, decay: 0.55, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 1.0, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.95, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.9, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.85, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.8, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.75, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.7, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.65, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.6, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:2, decay: 0.55, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 1.0, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.95, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.9, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.85, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.8, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.75, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.7, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.65, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.6, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, FirstVisit, episodes:3, decay: 0.55, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 1.0, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.95, params: 1.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.0 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.05 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.1 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.15 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.2 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.25 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.3 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.35 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.4 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.45 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.5 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.55 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.6 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.65 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.7 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.75 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.8 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.85 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.9 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n",
      "Results of the MC agent, EveryVisit, episodes:3, decay: 0.9, params: 0.95 batch-limit: 1000 policy-improvement: E-GREEDY :\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m [policy_improvement, batch_limit, episodes, visit_type, exploratory_decay, exploratory_param] \u001B[38;5;241m=\u001B[39m parameters\n\u001B[1;32m     27\u001B[0m mc_agent \u001B[38;5;241m=\u001B[39m MC_agent()\n\u001B[0;32m---> 28\u001B[0m mc_policy, mc_values, total_rewards, batches \u001B[38;5;241m=\u001B[39m \u001B[43mmc_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaze\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mFIRST_VISIT_MC\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvisit_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexploration_param\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexploratory_param\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexploration_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexploratory_decay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_improvement_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_improvement\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvisit_type_string\u001B[39m(is_first_visit):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_first_visit: \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirstVisit\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[0;32mIn[26], line 69\u001B[0m, in \u001B[0;36mMC_agent.solve\u001B[0;34m(self, env, FIRST_VISIT_MC, episodes_in_batch, exploration_param, exploration_decay, batch_limit, policy_improvement_type)\u001B[0m\n\u001B[1;32m     66\u001B[0m batches\u001B[38;5;241m.\u001B[39mappend(episodes)\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# First Visit Policy Evaluation\u001B[39;00m\n\u001B[0;32m---> 69\u001B[0m new_Q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_episodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfirst_visit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFIRST_VISIT_MC\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m mean_absolute_difference \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(np\u001B[38;5;241m.\u001B[39mabs(Q \u001B[38;5;241m-\u001B[39m new_Q))\n\u001B[1;32m     72\u001B[0m max_absolute_difference \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(np\u001B[38;5;241m.\u001B[39mabs(Q \u001B[38;5;241m-\u001B[39m new_Q))\n",
      "Cell \u001B[0;32mIn[26], line 164\u001B[0m, in \u001B[0;36mMC_agent.evaluate_episodes\u001B[0;34m(self, env, episodes, first_visit)\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate_episodes_first_visit(env, episodes)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_episodes_every_visit\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[26], line 154\u001B[0m, in \u001B[0;36mMC_agent.evaluate_episodes_every_visit\u001B[0;34m(self, env, episodes)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, episode_timestep \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(episode):\n\u001B[1;32m    152\u001B[0m   (current_timestep, current_state, action_picked, current_reward, _) \u001B[38;5;241m=\u001B[39m episode_timestep\n\u001B[0;32m--> 154\u001B[0m   value_of_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[43m[\u001B[49m\u001B[43melem\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_gamma\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43melem\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mepisode\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m    155\u001B[0m   _returns[current_state][action_picked]\u001B[38;5;241m.\u001B[39mappend(value_of_state)\n\u001B[1;32m    156\u001B[0m   _q[current_state][action_picked] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(_returns[current_state][action_picked])\n",
      "Cell \u001B[0;32mIn[26], line 154\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, episode_timestep \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(episode):\n\u001B[1;32m    152\u001B[0m   (current_timestep, current_state, action_picked, current_reward, _) \u001B[38;5;241m=\u001B[39m episode_timestep\n\u001B[0;32m--> 154\u001B[0m   value_of_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([elem[\u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m*\u001B[39m(\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_gamma\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mi) \u001B[38;5;28;01mfor\u001B[39;00m i, elem \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(episode[index:])])\n\u001B[1;32m    155\u001B[0m   _returns[current_state][action_picked]\u001B[38;5;241m.\u001B[39mappend(value_of_state)\n\u001B[1;32m    156\u001B[0m   _q[current_state][action_picked] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(_returns[current_state][action_picked])\n",
      "Cell \u001B[0;32mIn[4], line 195\u001B[0m, in \u001B[0;36mMaze.get_gamma\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_state_size\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    193\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_size\n\u001B[0;32m--> 195\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_gamma\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    196\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gamma\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# Functions used to perform episodes in the Maze environment\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "### Question 0: Defining the environment\n",
    "\n",
    "print(\"Creating the Maze:\\n\")\n",
    "maze = Maze()\n",
    "\n",
    "\n",
    "MC_Visits = [True, False] # First visit, every visit\n",
    "MC_Episodes_in_Batch = [1,2,3,4,5]\n",
    "MC_Exploration_Params = [ i/100 for i in range(0, 105, 5)]\n",
    "MC_Exploration_Decay = [ i / 1000 for i in range(1000, 950, -5)]\n",
    "MC_Batch_Limit = [i for i in range(1000, 10000, 1000)]\n",
    "MC_Policy_Improvement_Type = [\"E-GREEDY\", \"COMBINATION\", \"SOFT-MAX\"]\n",
    "\n",
    "combinations = []\n",
    "\n",
    "for policy_improvement in MC_Policy_Improvement_Type:\n",
    "    for batch_limit in MC_Batch_Limit:\n",
    "        for episodes in MC_Episodes_in_Batch:\n",
    "            for visit_type in MC_Visits:\n",
    "                for exploratory_decay in MC_Exploration_Decay:\n",
    "                    for exploratory_param in MC_Exploration_Params:\n",
    "                        combinations.append([policy_improvement, batch_limit, episodes, visit_type, exploratory_decay, exploratory_param])\n",
    "\n",
    "for parameters in combinations:\n",
    "    [policy_improvement, batch_limit, episodes, visit_type, exploratory_decay, exploratory_param] = parameters\n",
    "\n",
    "    mc_agent = MC_agent()\n",
    "    mc_policy, mc_values, total_rewards, batches = mc_agent.solve(maze, FIRST_VISIT_MC=visit_type, episodes_in_batch=episodes, exploration_param=exploratory_param, exploration_decay=exploratory_decay, batch_limit=batch_limit, policy_improvement_type=policy_improvement)\n",
    "\n",
    "    def visit_type_string(is_first_visit):\n",
    "        if is_first_visit: return \"FirstVisit\"\n",
    "        return \"EveryVisit\"\n",
    "\n",
    "    # test function\n",
    "    visualize_value_policy_and_hyperparameters(maze_env=maze, policy=mc_policy, V=mc_values[-1],  hyperparams=[policy_improvement, batch_limit, episodes, visit_type, exploratory_decay, exploratory_param], filename=f\"./images/MC/VFunction-{visit_type_string(visit_type)}-{episodes}-{exploratory_decay}-{exploratory_param}-{batch_limit}-{policy_improvement}.png\" )\n",
    "\n",
    "    print(f\"Results of the MC agent, {visit_type_string(visit_type)}, episodes:{episodes}, decay: {exploratory_decay}, params: {exploratory_param} batch-limit: {batch_limit} policy-improvement: {policy_improvement} :\\n\")\n",
    "    # maze.get_graphics().draw_policy(mc_policy)\n",
    "    # maze.get_graphics().draw_value(mc_values[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'Learning Rate')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_rewards)\n",
    "plt.title(\"Learning Rate over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning Rate\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Step 0/223\n",
      "Completed Step 1/223\n",
      "Completed Step 2/223\n",
      "Completed Step 3/223\n",
      "Completed Step 4/223\n",
      "Completed Step 5/223\n",
      "Completed Step 6/223\n",
      "Completed Step 7/223\n",
      "Completed Step 8/223\n",
      "Completed Step 9/223\n",
      "Completed Step 10/223\n",
      "Completed Step 11/223\n",
      "Completed Step 12/223\n",
      "Completed Step 13/223\n",
      "Completed Step 14/223\n",
      "Completed Step 15/223\n",
      "Completed Step 16/223\n",
      "Completed Step 17/223\n",
      "Completed Step 18/223\n",
      "Completed Step 19/223\n",
      "Completed Step 20/223\n",
      "Completed Step 21/223\n",
      "Completed Step 22/223\n",
      "Completed Step 23/223\n",
      "Completed Step 24/223\n",
      "Completed Step 25/223\n",
      "Completed Step 26/223\n",
      "Completed Step 27/223\n",
      "Completed Step 28/223\n",
      "Completed Step 29/223\n",
      "Completed Step 30/223\n",
      "Completed Step 31/223\n",
      "Completed Step 32/223\n",
      "Completed Step 33/223\n",
      "Completed Step 34/223\n",
      "Completed Step 35/223\n",
      "Completed Step 36/223\n",
      "Completed Step 37/223\n",
      "Completed Step 38/223\n",
      "Completed Step 39/223\n",
      "Completed Step 40/223\n",
      "Completed Step 41/223\n",
      "Completed Step 42/223\n",
      "Completed Step 43/223\n",
      "Completed Step 44/223\n",
      "Completed Step 45/223\n",
      "Completed Step 46/223\n",
      "Completed Step 47/223\n",
      "Completed Step 48/223\n",
      "Completed Step 49/223\n",
      "Completed Step 50/223\n",
      "Completed Step 51/223\n",
      "Completed Step 52/223\n",
      "Completed Step 53/223\n",
      "Completed Step 54/223\n",
      "Completed Step 55/223\n",
      "Completed Step 56/223\n",
      "Completed Step 57/223\n",
      "Completed Step 58/223\n",
      "Completed Step 59/223\n",
      "Completed Step 60/223\n",
      "Completed Step 61/223\n",
      "Completed Step 62/223\n",
      "Completed Step 63/223\n",
      "Completed Step 64/223\n",
      "Completed Step 65/223\n",
      "Completed Step 66/223\n",
      "Completed Step 67/223\n",
      "Completed Step 68/223\n",
      "Completed Step 69/223\n",
      "Completed Step 70/223\n",
      "Completed Step 71/223\n",
      "Completed Step 72/223\n",
      "Completed Step 73/223\n",
      "Completed Step 74/223\n",
      "Completed Step 75/223\n",
      "Completed Step 76/223\n",
      "Completed Step 77/223\n",
      "Completed Step 78/223\n",
      "Completed Step 79/223\n",
      "Completed Step 80/223\n",
      "Completed Step 81/223\n",
      "Completed Step 82/223\n",
      "Completed Step 83/223\n",
      "Completed Step 84/223\n",
      "Completed Step 85/223\n",
      "Completed Step 86/223\n",
      "Completed Step 87/223\n",
      "Completed Step 88/223\n",
      "Completed Step 89/223\n",
      "Completed Step 90/223\n",
      "Completed Step 91/223\n",
      "Completed Step 92/223\n",
      "Completed Step 93/223\n",
      "Completed Step 94/223\n",
      "Completed Step 95/223\n",
      "Completed Step 96/223\n",
      "Completed Step 97/223\n",
      "Completed Step 98/223\n",
      "Completed Step 99/223\n",
      "Completed Step 100/223\n",
      "Completed Step 101/223\n",
      "Completed Step 102/223\n",
      "Completed Step 103/223\n",
      "Completed Step 104/223\n",
      "Completed Step 105/223\n",
      "Completed Step 106/223\n",
      "Completed Step 107/223\n",
      "Completed Step 108/223\n",
      "Completed Step 109/223\n",
      "Completed Step 110/223\n",
      "Completed Step 111/223\n",
      "Completed Step 112/223\n",
      "Completed Step 113/223\n",
      "Completed Step 114/223\n",
      "Completed Step 115/223\n",
      "Completed Step 116/223\n",
      "Completed Step 117/223\n",
      "Completed Step 118/223\n",
      "Completed Step 119/223\n",
      "Completed Step 120/223\n",
      "Completed Step 121/223\n",
      "Completed Step 122/223\n",
      "Completed Step 123/223\n",
      "Completed Step 124/223\n",
      "Completed Step 125/223\n",
      "Completed Step 126/223\n",
      "Completed Step 127/223\n",
      "Completed Step 128/223\n",
      "Completed Step 129/223\n",
      "Completed Step 130/223\n",
      "Completed Step 131/223\n",
      "Completed Step 132/223\n",
      "Completed Step 133/223\n",
      "Completed Step 134/223\n",
      "Completed Step 135/223\n",
      "Completed Step 136/223\n",
      "Completed Step 137/223\n",
      "Completed Step 138/223\n",
      "Completed Step 139/223\n",
      "Completed Step 140/223\n",
      "Completed Step 141/223\n",
      "Completed Step 142/223\n",
      "Completed Step 143/223\n",
      "Completed Step 144/223\n",
      "Completed Step 145/223\n",
      "Completed Step 146/223\n",
      "Completed Step 147/223\n",
      "Completed Step 148/223\n",
      "Completed Step 149/223\n",
      "Completed Step 150/223\n",
      "Completed Step 151/223\n",
      "Completed Step 152/223\n",
      "Completed Step 153/223\n",
      "Completed Step 154/223\n",
      "Completed Step 155/223\n",
      "Completed Step 156/223\n",
      "Completed Step 157/223\n",
      "Completed Step 158/223\n",
      "Completed Step 159/223\n",
      "Completed Step 160/223\n",
      "Completed Step 161/223\n",
      "Completed Step 162/223\n",
      "Completed Step 163/223\n",
      "Completed Step 164/223\n",
      "Completed Step 165/223\n",
      "Completed Step 166/223\n",
      "Completed Step 167/223\n",
      "Completed Step 168/223\n",
      "Completed Step 169/223\n",
      "Completed Step 170/223\n",
      "Completed Step 171/223\n",
      "Completed Step 172/223\n",
      "Completed Step 173/223\n",
      "Completed Step 174/223\n",
      "Completed Step 175/223\n",
      "Completed Step 176/223\n",
      "Completed Step 177/223\n",
      "Completed Step 178/223\n",
      "Completed Step 179/223\n",
      "Completed Step 180/223\n",
      "Completed Step 181/223\n",
      "Completed Step 182/223\n",
      "Completed Step 183/223\n",
      "Completed Step 184/223\n",
      "Completed Step 185/223\n",
      "Completed Step 186/223\n",
      "Completed Step 187/223\n",
      "Completed Step 188/223\n",
      "Completed Step 189/223\n",
      "Completed Step 190/223\n",
      "Completed Step 191/223\n",
      "Completed Step 192/223\n",
      "Completed Step 193/223\n",
      "Completed Step 194/223\n",
      "Completed Step 195/223\n",
      "Completed Step 196/223\n",
      "Completed Step 197/223\n",
      "Completed Step 198/223\n",
      "Completed Step 199/223\n",
      "Completed Step 200/223\n",
      "Completed Step 201/223\n",
      "Completed Step 202/223\n",
      "Completed Step 203/223\n",
      "Completed Step 204/223\n",
      "Completed Step 205/223\n",
      "Completed Step 206/223\n",
      "Completed Step 207/223\n",
      "Completed Step 208/223\n",
      "Completed Step 209/223\n",
      "Completed Step 210/223\n",
      "Completed Step 211/223\n",
      "Completed Step 212/223\n",
      "Completed Step 213/223\n",
      "Completed Step 214/223\n",
      "Completed Step 215/223\n",
      "Completed Step 216/223\n",
      "Completed Step 217/223\n",
      "Completed Step 218/223\n",
      "Completed Step 219/223\n",
      "Completed Step 220/223\n",
      "Completed Step 221/223\n",
      "Completed Step 222/223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/74v_v08n4wxftghd_jwkbqt40000gn/T/ipykernel_27007/1656397134.py:52: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Step 0/230\n",
      "Completed Step 1/230\n",
      "Completed Step 2/230\n",
      "Completed Step 3/230\n",
      "Completed Step 4/230\n",
      "Completed Step 5/230\n",
      "Completed Step 6/230\n",
      "Completed Step 7/230\n",
      "Completed Step 8/230\n",
      "Completed Step 9/230\n",
      "Completed Step 10/230\n",
      "Completed Step 11/230\n",
      "Completed Step 12/230\n",
      "Completed Step 13/230\n",
      "Completed Step 14/230\n",
      "Completed Step 15/230\n",
      "Completed Step 16/230\n",
      "Completed Step 17/230\n",
      "Completed Step 18/230\n",
      "Completed Step 19/230\n",
      "Completed Step 20/230\n",
      "Completed Step 21/230\n",
      "Completed Step 22/230\n",
      "Completed Step 23/230\n",
      "Completed Step 24/230\n",
      "Completed Step 25/230\n",
      "Completed Step 26/230\n",
      "Completed Step 27/230\n",
      "Completed Step 28/230\n",
      "Completed Step 29/230\n",
      "Completed Step 30/230\n",
      "Completed Step 31/230\n",
      "Completed Step 32/230\n",
      "Completed Step 33/230\n",
      "Completed Step 34/230\n",
      "Completed Step 35/230\n",
      "Completed Step 36/230\n",
      "Completed Step 37/230\n",
      "Completed Step 38/230\n",
      "Completed Step 39/230\n",
      "Completed Step 40/230\n",
      "Completed Step 41/230\n",
      "Completed Step 42/230\n",
      "Completed Step 43/230\n",
      "Completed Step 44/230\n",
      "Completed Step 45/230\n",
      "Completed Step 46/230\n",
      "Completed Step 47/230\n",
      "Completed Step 48/230\n",
      "Completed Step 49/230\n",
      "Completed Step 50/230\n",
      "Completed Step 51/230\n",
      "Completed Step 52/230\n",
      "Completed Step 53/230\n",
      "Completed Step 54/230\n",
      "Completed Step 55/230\n",
      "Completed Step 56/230\n",
      "Completed Step 57/230\n",
      "Completed Step 58/230\n",
      "Completed Step 59/230\n",
      "Completed Step 60/230\n",
      "Completed Step 61/230\n",
      "Completed Step 62/230\n",
      "Completed Step 63/230\n",
      "Completed Step 64/230\n",
      "Completed Step 65/230\n",
      "Completed Step 66/230\n",
      "Completed Step 67/230\n",
      "Completed Step 68/230\n",
      "Completed Step 69/230\n",
      "Completed Step 70/230\n",
      "Completed Step 71/230\n",
      "Completed Step 72/230\n",
      "Completed Step 73/230\n",
      "Completed Step 74/230\n",
      "Completed Step 75/230\n",
      "Completed Step 76/230\n",
      "Completed Step 77/230\n",
      "Completed Step 78/230\n",
      "Completed Step 79/230\n",
      "Completed Step 80/230\n",
      "Completed Step 81/230\n",
      "Completed Step 82/230\n",
      "Completed Step 83/230\n",
      "Completed Step 84/230\n",
      "Completed Step 85/230\n",
      "Completed Step 86/230\n",
      "Completed Step 87/230\n",
      "Completed Step 88/230\n",
      "Completed Step 89/230\n",
      "Completed Step 90/230\n",
      "Completed Step 91/230\n",
      "Completed Step 92/230\n",
      "Completed Step 93/230\n",
      "Completed Step 94/230\n",
      "Completed Step 95/230\n",
      "Completed Step 96/230\n",
      "Completed Step 97/230\n",
      "Completed Step 98/230\n",
      "Completed Step 99/230\n",
      "Completed Step 100/230\n",
      "Completed Step 101/230\n",
      "Completed Step 102/230\n",
      "Completed Step 103/230\n",
      "Completed Step 104/230\n",
      "Completed Step 105/230\n",
      "Completed Step 106/230\n",
      "Completed Step 107/230\n",
      "Completed Step 108/230\n",
      "Completed Step 109/230\n",
      "Completed Step 110/230\n",
      "Completed Step 111/230\n",
      "Completed Step 112/230\n",
      "Completed Step 113/230\n",
      "Completed Step 114/230\n",
      "Completed Step 115/230\n",
      "Completed Step 116/230\n",
      "Completed Step 117/230\n",
      "Completed Step 118/230\n",
      "Completed Step 119/230\n",
      "Completed Step 120/230\n",
      "Completed Step 121/230\n",
      "Completed Step 122/230\n",
      "Completed Step 123/230\n",
      "Completed Step 124/230\n",
      "Completed Step 125/230\n",
      "Completed Step 126/230\n",
      "Completed Step 127/230\n",
      "Completed Step 128/230\n",
      "Completed Step 129/230\n",
      "Completed Step 130/230\n",
      "Completed Step 131/230\n",
      "Completed Step 132/230\n",
      "Completed Step 133/230\n",
      "Completed Step 134/230\n",
      "Completed Step 135/230\n",
      "Completed Step 136/230\n",
      "Completed Step 137/230\n",
      "Completed Step 138/230\n",
      "Completed Step 139/230\n",
      "Completed Step 140/230\n",
      "Completed Step 141/230\n",
      "Completed Step 142/230\n",
      "Completed Step 143/230\n",
      "Completed Step 144/230\n",
      "Completed Step 145/230\n",
      "Completed Step 146/230\n",
      "Completed Step 147/230\n",
      "Completed Step 148/230\n",
      "Completed Step 149/230\n",
      "Completed Step 150/230\n",
      "Completed Step 151/230\n",
      "Completed Step 152/230\n",
      "Completed Step 153/230\n",
      "Completed Step 154/230\n",
      "Completed Step 155/230\n",
      "Completed Step 156/230\n",
      "Completed Step 157/230\n",
      "Completed Step 158/230\n",
      "Completed Step 159/230\n",
      "Completed Step 160/230\n",
      "Completed Step 161/230\n",
      "Completed Step 162/230\n",
      "Completed Step 163/230\n",
      "Completed Step 164/230\n",
      "Completed Step 165/230\n",
      "Completed Step 166/230\n",
      "Completed Step 167/230\n",
      "Completed Step 168/230\n",
      "Completed Step 169/230\n",
      "Completed Step 170/230\n",
      "Completed Step 171/230\n",
      "Completed Step 172/230\n",
      "Completed Step 173/230\n",
      "Completed Step 174/230\n",
      "Completed Step 175/230\n",
      "Completed Step 176/230\n",
      "Completed Step 177/230\n",
      "Completed Step 178/230\n",
      "Completed Step 179/230\n",
      "Completed Step 180/230\n",
      "Completed Step 181/230\n",
      "Completed Step 182/230\n",
      "Completed Step 183/230\n",
      "Completed Step 184/230\n",
      "Completed Step 185/230\n",
      "Completed Step 186/230\n",
      "Completed Step 187/230\n",
      "Completed Step 188/230\n",
      "Completed Step 189/230\n",
      "Completed Step 190/230\n",
      "Completed Step 191/230\n",
      "Completed Step 192/230\n",
      "Completed Step 193/230\n",
      "Completed Step 194/230\n",
      "Completed Step 195/230\n",
      "Completed Step 196/230\n",
      "Completed Step 197/230\n",
      "Completed Step 198/230\n",
      "Completed Step 199/230\n",
      "Completed Step 200/230\n",
      "Completed Step 201/230\n",
      "Completed Step 202/230\n",
      "Completed Step 203/230\n",
      "Completed Step 204/230\n",
      "Completed Step 205/230\n",
      "Completed Step 206/230\n",
      "Completed Step 207/230\n",
      "Completed Step 208/230\n",
      "Completed Step 209/230\n",
      "Completed Step 210/230\n",
      "Completed Step 211/230\n",
      "Completed Step 212/230\n",
      "Completed Step 213/230\n",
      "Completed Step 214/230\n",
      "Completed Step 215/230\n",
      "Completed Step 216/230\n",
      "Completed Step 217/230\n",
      "Completed Step 218/230\n",
      "Completed Step 219/230\n",
      "Completed Step 220/230\n",
      "Completed Step 221/230\n",
      "Completed Step 222/230\n",
      "Completed Step 223/230\n",
      "Completed Step 224/230\n",
      "Completed Step 225/230\n",
      "Completed Step 226/230\n",
      "Completed Step 227/230\n",
      "Completed Step 228/230\n",
      "Completed Step 229/230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/74v_v08n4wxftghd_jwkbqt40000gn/T/ipykernel_27007/1656397134.py:52: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Step 0/129\n",
      "Completed Step 1/129\n",
      "Completed Step 2/129\n",
      "Completed Step 3/129\n",
      "Completed Step 4/129\n",
      "Completed Step 5/129\n",
      "Completed Step 6/129\n",
      "Completed Step 7/129\n",
      "Completed Step 8/129\n",
      "Completed Step 9/129\n",
      "Completed Step 10/129\n",
      "Completed Step 11/129\n",
      "Completed Step 12/129\n",
      "Completed Step 13/129\n",
      "Completed Step 14/129\n",
      "Completed Step 15/129\n",
      "Completed Step 16/129\n",
      "Completed Step 17/129\n",
      "Completed Step 18/129\n",
      "Completed Step 19/129\n",
      "Completed Step 20/129\n",
      "Completed Step 21/129\n",
      "Completed Step 22/129\n",
      "Completed Step 23/129\n",
      "Completed Step 24/129\n",
      "Completed Step 25/129\n",
      "Completed Step 26/129\n",
      "Completed Step 27/129\n",
      "Completed Step 28/129\n",
      "Completed Step 29/129\n",
      "Completed Step 30/129\n",
      "Completed Step 31/129\n",
      "Completed Step 32/129\n",
      "Completed Step 33/129\n",
      "Completed Step 34/129\n",
      "Completed Step 35/129\n",
      "Completed Step 36/129\n",
      "Completed Step 37/129\n",
      "Completed Step 38/129\n",
      "Completed Step 39/129\n",
      "Completed Step 40/129\n",
      "Completed Step 41/129\n",
      "Completed Step 42/129\n",
      "Completed Step 43/129\n",
      "Completed Step 44/129\n",
      "Completed Step 45/129\n",
      "Completed Step 46/129\n",
      "Completed Step 47/129\n",
      "Completed Step 48/129\n",
      "Completed Step 49/129\n",
      "Completed Step 50/129\n",
      "Completed Step 51/129\n",
      "Completed Step 52/129\n",
      "Completed Step 53/129\n",
      "Completed Step 54/129\n",
      "Completed Step 55/129\n",
      "Completed Step 56/129\n",
      "Completed Step 57/129\n",
      "Completed Step 58/129\n",
      "Completed Step 59/129\n",
      "Completed Step 60/129\n",
      "Completed Step 61/129\n",
      "Completed Step 62/129\n",
      "Completed Step 63/129\n",
      "Completed Step 64/129\n",
      "Completed Step 65/129\n",
      "Completed Step 66/129\n",
      "Completed Step 67/129\n",
      "Completed Step 68/129\n",
      "Completed Step 69/129\n",
      "Completed Step 70/129\n",
      "Completed Step 71/129\n",
      "Completed Step 72/129\n",
      "Completed Step 73/129\n",
      "Completed Step 74/129\n",
      "Completed Step 75/129\n",
      "Completed Step 76/129\n",
      "Completed Step 77/129\n",
      "Completed Step 78/129\n",
      "Completed Step 79/129\n",
      "Completed Step 80/129\n",
      "Completed Step 81/129\n",
      "Completed Step 82/129\n",
      "Completed Step 83/129\n",
      "Completed Step 84/129\n",
      "Completed Step 85/129\n",
      "Completed Step 86/129\n",
      "Completed Step 87/129\n",
      "Completed Step 88/129\n",
      "Completed Step 89/129\n",
      "Completed Step 90/129\n",
      "Completed Step 91/129\n",
      "Completed Step 92/129\n",
      "Completed Step 93/129\n",
      "Completed Step 94/129\n",
      "Completed Step 95/129\n",
      "Completed Step 96/129\n",
      "Completed Step 97/129\n",
      "Completed Step 98/129\n",
      "Completed Step 99/129\n",
      "Completed Step 100/129\n",
      "Completed Step 101/129\n",
      "Completed Step 102/129\n",
      "Completed Step 103/129\n",
      "Completed Step 104/129\n",
      "Completed Step 105/129\n",
      "Completed Step 106/129\n",
      "Completed Step 107/129\n",
      "Completed Step 108/129\n",
      "Completed Step 109/129\n",
      "Completed Step 110/129\n",
      "Completed Step 111/129\n",
      "Completed Step 112/129\n",
      "Completed Step 113/129\n",
      "Completed Step 114/129\n",
      "Completed Step 115/129\n",
      "Completed Step 116/129\n",
      "Completed Step 117/129\n",
      "Completed Step 118/129\n",
      "Completed Step 119/129\n",
      "Completed Step 120/129\n",
      "Completed Step 121/129\n",
      "Completed Step 122/129\n",
      "Completed Step 123/129\n",
      "Completed Step 124/129\n",
      "Completed Step 125/129\n",
      "Completed Step 126/129\n",
      "Completed Step 127/129\n",
      "Completed Step 128/129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/74v_v08n4wxftghd_jwkbqt40000gn/T/ipykernel_27007/1656397134.py:52: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Step 0/502\n",
      "Completed Step 1/502\n",
      "Completed Step 2/502\n",
      "Completed Step 3/502\n",
      "Completed Step 4/502\n",
      "Completed Step 5/502\n",
      "Completed Step 6/502\n",
      "Completed Step 7/502\n",
      "Completed Step 8/502\n",
      "Completed Step 9/502\n",
      "Completed Step 10/502\n",
      "Completed Step 11/502\n",
      "Completed Step 12/502\n",
      "Completed Step 13/502\n",
      "Completed Step 14/502\n",
      "Completed Step 15/502\n",
      "Completed Step 16/502\n",
      "Completed Step 17/502\n",
      "Completed Step 18/502\n",
      "Completed Step 19/502\n",
      "Completed Step 20/502\n",
      "Completed Step 21/502\n",
      "Completed Step 22/502\n",
      "Completed Step 23/502\n",
      "Completed Step 24/502\n",
      "Completed Step 25/502\n",
      "Completed Step 26/502\n",
      "Completed Step 27/502\n",
      "Completed Step 28/502\n",
      "Completed Step 29/502\n",
      "Completed Step 30/502\n",
      "Completed Step 31/502\n",
      "Completed Step 32/502\n",
      "Completed Step 33/502\n",
      "Completed Step 34/502\n",
      "Completed Step 35/502\n",
      "Completed Step 36/502\n",
      "Completed Step 37/502\n",
      "Completed Step 38/502\n",
      "Completed Step 39/502\n",
      "Completed Step 40/502\n",
      "Completed Step 41/502\n",
      "Completed Step 42/502\n",
      "Completed Step 43/502\n",
      "Completed Step 44/502\n",
      "Completed Step 45/502\n",
      "Completed Step 46/502\n",
      "Completed Step 47/502\n",
      "Completed Step 48/502\n",
      "Completed Step 49/502\n",
      "Completed Step 50/502\n",
      "Completed Step 51/502\n",
      "Completed Step 52/502\n",
      "Completed Step 53/502\n",
      "Completed Step 54/502\n",
      "Completed Step 55/502\n",
      "Completed Step 56/502\n",
      "Completed Step 57/502\n",
      "Completed Step 58/502\n",
      "Completed Step 59/502\n",
      "Completed Step 60/502\n",
      "Completed Step 61/502\n",
      "Completed Step 62/502\n",
      "Completed Step 63/502\n",
      "Completed Step 64/502\n",
      "Completed Step 65/502\n",
      "Completed Step 66/502\n",
      "Completed Step 67/502\n",
      "Completed Step 68/502\n",
      "Completed Step 69/502\n",
      "Completed Step 70/502\n",
      "Completed Step 71/502\n",
      "Completed Step 72/502\n",
      "Completed Step 73/502\n",
      "Completed Step 74/502\n",
      "Completed Step 75/502\n",
      "Completed Step 76/502\n",
      "Completed Step 77/502\n",
      "Completed Step 78/502\n",
      "Completed Step 79/502\n",
      "Completed Step 80/502\n",
      "Completed Step 81/502\n",
      "Completed Step 82/502\n",
      "Completed Step 83/502\n",
      "Completed Step 84/502\n",
      "Completed Step 85/502\n",
      "Completed Step 86/502\n",
      "Completed Step 87/502\n",
      "Completed Step 88/502\n",
      "Completed Step 89/502\n",
      "Completed Step 90/502\n",
      "Completed Step 91/502\n",
      "Completed Step 92/502\n",
      "Completed Step 93/502\n",
      "Completed Step 94/502\n",
      "Completed Step 95/502\n",
      "Completed Step 96/502\n",
      "Completed Step 97/502\n",
      "Completed Step 98/502\n",
      "Completed Step 99/502\n",
      "Completed Step 100/502\n",
      "Completed Step 101/502\n",
      "Completed Step 102/502\n",
      "Completed Step 103/502\n",
      "Completed Step 104/502\n",
      "Completed Step 105/502\n",
      "Completed Step 106/502\n",
      "Completed Step 107/502\n",
      "Completed Step 108/502\n",
      "Completed Step 109/502\n",
      "Completed Step 110/502\n",
      "Completed Step 111/502\n",
      "Completed Step 112/502\n",
      "Completed Step 113/502\n",
      "Completed Step 114/502\n",
      "Completed Step 115/502\n",
      "Completed Step 116/502\n",
      "Completed Step 117/502\n",
      "Completed Step 118/502\n",
      "Completed Step 119/502\n",
      "Completed Step 120/502\n",
      "Completed Step 121/502\n",
      "Completed Step 122/502\n",
      "Completed Step 123/502\n",
      "Completed Step 124/502\n",
      "Completed Step 125/502\n",
      "Completed Step 126/502\n",
      "Completed Step 127/502\n",
      "Completed Step 128/502\n",
      "Completed Step 129/502\n",
      "Completed Step 130/502\n",
      "Completed Step 131/502\n",
      "Completed Step 132/502\n",
      "Completed Step 133/502\n",
      "Completed Step 134/502\n",
      "Completed Step 135/502\n",
      "Completed Step 136/502\n",
      "Completed Step 137/502\n",
      "Completed Step 138/502\n",
      "Completed Step 139/502\n",
      "Completed Step 140/502\n",
      "Completed Step 141/502\n",
      "Completed Step 142/502\n",
      "Completed Step 143/502\n",
      "Completed Step 144/502\n",
      "Completed Step 145/502\n",
      "Completed Step 146/502\n",
      "Completed Step 147/502\n",
      "Completed Step 148/502\n",
      "Completed Step 149/502\n",
      "Completed Step 150/502\n",
      "Completed Step 151/502\n",
      "Completed Step 152/502\n",
      "Completed Step 153/502\n",
      "Completed Step 154/502\n",
      "Completed Step 155/502\n",
      "Completed Step 156/502\n",
      "Completed Step 157/502\n",
      "Completed Step 158/502\n",
      "Completed Step 159/502\n",
      "Completed Step 160/502\n",
      "Completed Step 161/502\n",
      "Completed Step 162/502\n",
      "Completed Step 163/502\n",
      "Completed Step 164/502\n",
      "Completed Step 165/502\n",
      "Completed Step 166/502\n",
      "Completed Step 167/502\n",
      "Completed Step 168/502\n",
      "Completed Step 169/502\n",
      "Completed Step 170/502\n",
      "Completed Step 171/502\n",
      "Completed Step 172/502\n",
      "Completed Step 173/502\n",
      "Completed Step 174/502\n",
      "Completed Step 175/502\n",
      "Completed Step 176/502\n",
      "Completed Step 177/502\n",
      "Completed Step 178/502\n",
      "Completed Step 179/502\n",
      "Completed Step 180/502\n",
      "Completed Step 181/502\n",
      "Completed Step 182/502\n",
      "Completed Step 183/502\n",
      "Completed Step 184/502\n",
      "Completed Step 185/502\n",
      "Completed Step 186/502\n",
      "Completed Step 187/502\n",
      "Completed Step 188/502\n",
      "Completed Step 189/502\n",
      "Completed Step 190/502\n",
      "Completed Step 191/502\n",
      "Completed Step 192/502\n",
      "Completed Step 193/502\n",
      "Completed Step 194/502\n",
      "Completed Step 195/502\n",
      "Completed Step 196/502\n",
      "Completed Step 197/502\n",
      "Completed Step 198/502\n",
      "Completed Step 199/502\n",
      "Completed Step 200/502\n",
      "Completed Step 201/502\n",
      "Completed Step 202/502\n",
      "Completed Step 203/502\n",
      "Completed Step 204/502\n",
      "Completed Step 205/502\n",
      "Completed Step 206/502\n",
      "Completed Step 207/502\n",
      "Completed Step 208/502\n",
      "Completed Step 209/502\n",
      "Completed Step 210/502\n",
      "Completed Step 211/502\n",
      "Completed Step 212/502\n",
      "Completed Step 213/502\n",
      "Completed Step 214/502\n",
      "Completed Step 215/502\n",
      "Completed Step 216/502\n",
      "Completed Step 217/502\n",
      "Completed Step 218/502\n",
      "Completed Step 219/502\n",
      "Completed Step 220/502\n",
      "Completed Step 221/502\n",
      "Completed Step 222/502\n",
      "Completed Step 223/502\n",
      "Completed Step 224/502\n",
      "Completed Step 225/502\n",
      "Completed Step 226/502\n",
      "Completed Step 227/502\n",
      "Completed Step 228/502\n",
      "Completed Step 229/502\n",
      "Completed Step 230/502\n",
      "Completed Step 231/502\n",
      "Completed Step 232/502\n",
      "Completed Step 233/502\n",
      "Completed Step 234/502\n",
      "Completed Step 235/502\n",
      "Completed Step 236/502\n",
      "Completed Step 237/502\n",
      "Completed Step 238/502\n",
      "Completed Step 239/502\n",
      "Completed Step 240/502\n",
      "Completed Step 241/502\n",
      "Completed Step 242/502\n",
      "Completed Step 243/502\n",
      "Completed Step 244/502\n",
      "Completed Step 245/502\n",
      "Completed Step 246/502\n",
      "Completed Step 247/502\n",
      "Completed Step 248/502\n",
      "Completed Step 249/502\n",
      "Completed Step 250/502\n",
      "Completed Step 251/502\n",
      "Completed Step 252/502\n",
      "Completed Step 253/502\n",
      "Completed Step 254/502\n",
      "Completed Step 255/502\n",
      "Completed Step 256/502\n",
      "Completed Step 257/502\n",
      "Completed Step 258/502\n",
      "Completed Step 259/502\n",
      "Completed Step 260/502\n",
      "Completed Step 261/502\n",
      "Completed Step 262/502\n",
      "Completed Step 263/502\n",
      "Completed Step 264/502\n",
      "Completed Step 265/502\n",
      "Completed Step 266/502\n",
      "Completed Step 267/502\n",
      "Completed Step 268/502\n",
      "Completed Step 269/502\n",
      "Completed Step 270/502\n",
      "Completed Step 271/502\n",
      "Completed Step 272/502\n",
      "Completed Step 273/502\n",
      "Completed Step 274/502\n",
      "Completed Step 275/502\n",
      "Completed Step 276/502\n",
      "Completed Step 277/502\n",
      "Completed Step 278/502\n",
      "Completed Step 279/502\n",
      "Completed Step 280/502\n",
      "Completed Step 281/502\n",
      "Completed Step 282/502\n",
      "Completed Step 283/502\n",
      "Completed Step 284/502\n",
      "Completed Step 285/502\n",
      "Completed Step 286/502\n",
      "Completed Step 287/502\n",
      "Completed Step 288/502\n",
      "Completed Step 289/502\n",
      "Completed Step 290/502\n",
      "Completed Step 291/502\n",
      "Completed Step 292/502\n",
      "Completed Step 293/502\n",
      "Completed Step 294/502\n",
      "Completed Step 295/502\n",
      "Completed Step 296/502\n",
      "Completed Step 297/502\n",
      "Completed Step 298/502\n",
      "Completed Step 299/502\n",
      "Completed Step 300/502\n",
      "Completed Step 301/502\n",
      "Completed Step 302/502\n",
      "Completed Step 303/502\n",
      "Completed Step 304/502\n",
      "Completed Step 305/502\n",
      "Completed Step 306/502\n",
      "Completed Step 307/502\n",
      "Completed Step 308/502\n",
      "Completed Step 309/502\n",
      "Completed Step 310/502\n",
      "Completed Step 311/502\n",
      "Completed Step 312/502\n",
      "Completed Step 313/502\n",
      "Completed Step 314/502\n",
      "Completed Step 315/502\n",
      "Completed Step 316/502\n",
      "Completed Step 317/502\n",
      "Completed Step 318/502\n",
      "Completed Step 319/502\n",
      "Completed Step 320/502\n",
      "Completed Step 321/502\n",
      "Completed Step 322/502\n",
      "Completed Step 323/502\n",
      "Completed Step 324/502\n",
      "Completed Step 325/502\n",
      "Completed Step 326/502\n",
      "Completed Step 327/502\n",
      "Completed Step 328/502\n",
      "Completed Step 329/502\n",
      "Completed Step 330/502\n",
      "Completed Step 331/502\n",
      "Completed Step 332/502\n",
      "Completed Step 333/502\n",
      "Completed Step 334/502\n",
      "Completed Step 335/502\n",
      "Completed Step 336/502\n",
      "Completed Step 337/502\n",
      "Completed Step 338/502\n",
      "Completed Step 339/502\n",
      "Completed Step 340/502\n",
      "Completed Step 341/502\n",
      "Completed Step 342/502\n",
      "Completed Step 343/502\n",
      "Completed Step 344/502\n",
      "Completed Step 345/502\n",
      "Completed Step 346/502\n",
      "Completed Step 347/502\n",
      "Completed Step 348/502\n",
      "Completed Step 349/502\n",
      "Completed Step 350/502\n",
      "Completed Step 351/502\n",
      "Completed Step 352/502\n",
      "Completed Step 353/502\n",
      "Completed Step 354/502\n",
      "Completed Step 355/502\n",
      "Completed Step 356/502\n",
      "Completed Step 357/502\n",
      "Completed Step 358/502\n",
      "Completed Step 359/502\n",
      "Completed Step 360/502\n",
      "Completed Step 361/502\n",
      "Completed Step 362/502\n",
      "Completed Step 363/502\n",
      "Completed Step 364/502\n",
      "Completed Step 365/502\n",
      "Completed Step 366/502\n",
      "Completed Step 367/502\n",
      "Completed Step 368/502\n",
      "Completed Step 369/502\n",
      "Completed Step 370/502\n",
      "Completed Step 371/502\n",
      "Completed Step 372/502\n",
      "Completed Step 373/502\n",
      "Completed Step 374/502\n",
      "Completed Step 375/502\n",
      "Completed Step 376/502\n",
      "Completed Step 377/502\n",
      "Completed Step 378/502\n",
      "Completed Step 379/502\n",
      "Completed Step 380/502\n",
      "Completed Step 381/502\n",
      "Completed Step 382/502\n",
      "Completed Step 383/502\n",
      "Completed Step 384/502\n",
      "Completed Step 385/502\n",
      "Completed Step 386/502\n",
      "Completed Step 387/502\n",
      "Completed Step 388/502\n",
      "Completed Step 389/502\n",
      "Completed Step 390/502\n",
      "Completed Step 391/502\n",
      "Completed Step 392/502\n",
      "Completed Step 393/502\n",
      "Completed Step 394/502\n",
      "Completed Step 395/502\n",
      "Completed Step 396/502\n",
      "Completed Step 397/502\n",
      "Completed Step 398/502\n",
      "Completed Step 399/502\n",
      "Completed Step 400/502\n",
      "Completed Step 401/502\n",
      "Completed Step 402/502\n",
      "Completed Step 403/502\n",
      "Completed Step 404/502\n",
      "Completed Step 405/502\n",
      "Completed Step 406/502\n",
      "Completed Step 407/502\n",
      "Completed Step 408/502\n",
      "Completed Step 409/502\n",
      "Completed Step 410/502\n",
      "Completed Step 411/502\n",
      "Completed Step 412/502\n",
      "Completed Step 413/502\n",
      "Completed Step 414/502\n",
      "Completed Step 415/502\n",
      "Completed Step 416/502\n",
      "Completed Step 417/502\n",
      "Completed Step 418/502\n",
      "Completed Step 419/502\n",
      "Completed Step 420/502\n",
      "Completed Step 421/502\n",
      "Completed Step 422/502\n",
      "Completed Step 423/502\n",
      "Completed Step 424/502\n",
      "Completed Step 425/502\n",
      "Completed Step 426/502\n",
      "Completed Step 427/502\n",
      "Completed Step 428/502\n",
      "Completed Step 429/502\n",
      "Completed Step 430/502\n",
      "Completed Step 431/502\n",
      "Completed Step 432/502\n",
      "Completed Step 433/502\n",
      "Completed Step 434/502\n",
      "Completed Step 435/502\n",
      "Completed Step 436/502\n",
      "Completed Step 437/502\n",
      "Completed Step 438/502\n",
      "Completed Step 439/502\n",
      "Completed Step 440/502\n",
      "Completed Step 441/502\n",
      "Completed Step 442/502\n",
      "Completed Step 443/502\n",
      "Completed Step 444/502\n",
      "Completed Step 445/502\n",
      "Completed Step 446/502\n",
      "Completed Step 447/502\n",
      "Completed Step 448/502\n",
      "Completed Step 449/502\n",
      "Completed Step 450/502\n",
      "Completed Step 451/502\n",
      "Completed Step 452/502\n",
      "Completed Step 453/502\n",
      "Completed Step 454/502\n",
      "Completed Step 455/502\n",
      "Completed Step 456/502\n",
      "Completed Step 457/502\n",
      "Completed Step 458/502\n",
      "Completed Step 459/502\n",
      "Completed Step 460/502\n",
      "Completed Step 461/502\n",
      "Completed Step 462/502\n",
      "Completed Step 463/502\n",
      "Completed Step 464/502\n",
      "Completed Step 465/502\n",
      "Completed Step 466/502\n",
      "Completed Step 467/502\n",
      "Completed Step 468/502\n",
      "Completed Step 469/502\n",
      "Completed Step 470/502\n",
      "Completed Step 471/502\n",
      "Completed Step 472/502\n",
      "Completed Step 473/502\n",
      "Completed Step 474/502\n",
      "Completed Step 475/502\n",
      "Completed Step 476/502\n",
      "Completed Step 477/502\n",
      "Completed Step 478/502\n",
      "Completed Step 479/502\n",
      "Completed Step 480/502\n",
      "Completed Step 481/502\n",
      "Completed Step 482/502\n",
      "Completed Step 483/502\n",
      "Completed Step 484/502\n",
      "Completed Step 485/502\n",
      "Completed Step 486/502\n",
      "Completed Step 487/502\n",
      "Completed Step 488/502\n",
      "Completed Step 489/502\n",
      "Completed Step 490/502\n",
      "Completed Step 491/502\n",
      "Completed Step 492/502\n",
      "Completed Step 493/502\n",
      "Completed Step 494/502\n",
      "Completed Step 495/502\n",
      "Completed Step 496/502\n",
      "Completed Step 497/502\n",
      "Completed Step 498/502\n",
      "Completed Step 499/502\n",
      "Completed Step 500/502\n",
      "Completed Step 501/502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/74v_v08n4wxftghd_jwkbqt40000gn/T/ipykernel_27007/1656397134.py:52: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Step 0/229\n",
      "Completed Step 1/229\n",
      "Completed Step 2/229\n",
      "Completed Step 3/229\n",
      "Completed Step 4/229\n",
      "Completed Step 5/229\n",
      "Completed Step 6/229\n",
      "Completed Step 7/229\n",
      "Completed Step 8/229\n",
      "Completed Step 9/229\n",
      "Completed Step 10/229\n",
      "Completed Step 11/229\n",
      "Completed Step 12/229\n",
      "Completed Step 13/229\n",
      "Completed Step 14/229\n",
      "Completed Step 15/229\n",
      "Completed Step 16/229\n",
      "Completed Step 17/229\n",
      "Completed Step 18/229\n",
      "Completed Step 19/229\n",
      "Completed Step 20/229\n",
      "Completed Step 21/229\n",
      "Completed Step 22/229\n",
      "Completed Step 23/229\n",
      "Completed Step 24/229\n",
      "Completed Step 25/229\n",
      "Completed Step 26/229\n",
      "Completed Step 27/229\n",
      "Completed Step 28/229\n",
      "Completed Step 29/229\n",
      "Completed Step 30/229\n",
      "Completed Step 31/229\n",
      "Completed Step 32/229\n",
      "Completed Step 33/229\n",
      "Completed Step 34/229\n",
      "Completed Step 35/229\n",
      "Completed Step 36/229\n",
      "Completed Step 37/229\n",
      "Completed Step 38/229\n",
      "Completed Step 39/229\n",
      "Completed Step 40/229\n",
      "Completed Step 41/229\n",
      "Completed Step 42/229\n",
      "Completed Step 43/229\n",
      "Completed Step 44/229\n",
      "Completed Step 45/229\n",
      "Completed Step 46/229\n",
      "Completed Step 47/229\n",
      "Completed Step 48/229\n",
      "Completed Step 49/229\n",
      "Completed Step 50/229\n",
      "Completed Step 51/229\n",
      "Completed Step 52/229\n",
      "Completed Step 53/229\n",
      "Completed Step 54/229\n",
      "Completed Step 55/229\n",
      "Completed Step 56/229\n",
      "Completed Step 57/229\n",
      "Completed Step 58/229\n",
      "Completed Step 59/229\n",
      "Completed Step 60/229\n",
      "Completed Step 61/229\n",
      "Completed Step 62/229\n",
      "Completed Step 63/229\n",
      "Completed Step 64/229\n",
      "Completed Step 65/229\n",
      "Completed Step 66/229\n",
      "Completed Step 67/229\n",
      "Completed Step 68/229\n",
      "Completed Step 69/229\n",
      "Completed Step 70/229\n",
      "Completed Step 71/229\n",
      "Completed Step 72/229\n",
      "Completed Step 73/229\n",
      "Completed Step 74/229\n",
      "Completed Step 75/229\n",
      "Completed Step 76/229\n",
      "Completed Step 77/229\n",
      "Completed Step 78/229\n",
      "Completed Step 79/229\n",
      "Completed Step 80/229\n",
      "Completed Step 81/229\n",
      "Completed Step 82/229\n",
      "Completed Step 83/229\n",
      "Completed Step 84/229\n",
      "Completed Step 85/229\n",
      "Completed Step 86/229\n",
      "Completed Step 87/229\n",
      "Completed Step 88/229\n",
      "Completed Step 89/229\n",
      "Completed Step 90/229\n",
      "Completed Step 91/229\n",
      "Completed Step 92/229\n",
      "Completed Step 93/229\n",
      "Completed Step 94/229\n",
      "Completed Step 95/229\n",
      "Completed Step 96/229\n",
      "Completed Step 97/229\n",
      "Completed Step 98/229\n",
      "Completed Step 99/229\n",
      "Completed Step 100/229\n",
      "Completed Step 101/229\n",
      "Completed Step 102/229\n",
      "Completed Step 103/229\n",
      "Completed Step 104/229\n",
      "Completed Step 105/229\n",
      "Completed Step 106/229\n",
      "Completed Step 107/229\n",
      "Completed Step 108/229\n",
      "Completed Step 109/229\n",
      "Completed Step 110/229\n",
      "Completed Step 111/229\n",
      "Completed Step 112/229\n",
      "Completed Step 113/229\n",
      "Completed Step 114/229\n",
      "Completed Step 115/229\n",
      "Completed Step 116/229\n",
      "Completed Step 117/229\n",
      "Completed Step 118/229\n",
      "Completed Step 119/229\n",
      "Completed Step 120/229\n",
      "Completed Step 121/229\n",
      "Completed Step 122/229\n",
      "Completed Step 123/229\n",
      "Completed Step 124/229\n",
      "Completed Step 125/229\n",
      "Completed Step 126/229\n",
      "Completed Step 127/229\n",
      "Completed Step 128/229\n",
      "Completed Step 129/229\n",
      "Completed Step 130/229\n",
      "Completed Step 131/229\n",
      "Completed Step 132/229\n",
      "Completed Step 133/229\n",
      "Completed Step 134/229\n",
      "Completed Step 135/229\n",
      "Completed Step 136/229\n",
      "Completed Step 137/229\n",
      "Completed Step 138/229\n",
      "Completed Step 139/229\n",
      "Completed Step 140/229\n",
      "Completed Step 141/229\n",
      "Completed Step 142/229\n",
      "Completed Step 143/229\n",
      "Completed Step 144/229\n",
      "Completed Step 145/229\n",
      "Completed Step 146/229\n",
      "Completed Step 147/229\n",
      "Completed Step 148/229\n",
      "Completed Step 149/229\n",
      "Completed Step 150/229\n",
      "Completed Step 151/229\n",
      "Completed Step 152/229\n",
      "Completed Step 153/229\n",
      "Completed Step 154/229\n",
      "Completed Step 155/229\n",
      "Completed Step 156/229\n",
      "Completed Step 157/229\n",
      "Completed Step 158/229\n",
      "Completed Step 159/229\n",
      "Completed Step 160/229\n",
      "Completed Step 161/229\n",
      "Completed Step 162/229\n",
      "Completed Step 163/229\n",
      "Completed Step 164/229\n",
      "Completed Step 165/229\n",
      "Completed Step 166/229\n",
      "Completed Step 167/229\n",
      "Completed Step 168/229\n",
      "Completed Step 169/229\n",
      "Completed Step 170/229\n",
      "Completed Step 171/229\n",
      "Completed Step 172/229\n",
      "Completed Step 173/229\n",
      "Completed Step 174/229\n",
      "Completed Step 175/229\n",
      "Completed Step 176/229\n",
      "Completed Step 177/229\n",
      "Completed Step 178/229\n",
      "Completed Step 179/229\n",
      "Completed Step 180/229\n",
      "Completed Step 181/229\n",
      "Completed Step 182/229\n",
      "Completed Step 183/229\n",
      "Completed Step 184/229\n",
      "Completed Step 185/229\n",
      "Completed Step 186/229\n",
      "Completed Step 187/229\n",
      "Completed Step 188/229\n",
      "Completed Step 189/229\n",
      "Completed Step 190/229\n",
      "Completed Step 191/229\n",
      "Completed Step 192/229\n",
      "Completed Step 193/229\n",
      "Completed Step 194/229\n",
      "Completed Step 195/229\n",
      "Completed Step 196/229\n",
      "Completed Step 197/229\n",
      "Completed Step 198/229\n",
      "Completed Step 199/229\n",
      "Completed Step 200/229\n",
      "Completed Step 201/229\n",
      "Completed Step 202/229\n",
      "Completed Step 203/229\n",
      "Completed Step 204/229\n",
      "Completed Step 205/229\n",
      "Completed Step 206/229\n",
      "Completed Step 207/229\n",
      "Completed Step 208/229\n",
      "Completed Step 209/229\n",
      "Completed Step 210/229\n",
      "Completed Step 211/229\n",
      "Completed Step 212/229\n",
      "Completed Step 213/229\n",
      "Completed Step 214/229\n",
      "Completed Step 215/229\n",
      "Completed Step 216/229\n",
      "Completed Step 217/229\n",
      "Completed Step 218/229\n",
      "Completed Step 219/229\n",
      "Completed Step 220/229\n",
      "Completed Step 221/229\n",
      "Completed Step 222/229\n",
      "Completed Step 223/229\n",
      "Completed Step 224/229\n",
      "Completed Step 225/229\n",
      "Completed Step 226/229\n",
      "Completed Step 227/229\n",
      "Completed Step 228/229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/74v_v08n4wxftghd_jwkbqt40000gn/T/ipykernel_27007/1656397134.py:52: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Step 0/422\n",
      "Completed Step 1/422\n",
      "Completed Step 2/422\n",
      "Completed Step 3/422\n",
      "Completed Step 4/422\n",
      "Completed Step 5/422\n",
      "Completed Step 6/422\n",
      "Completed Step 7/422\n",
      "Completed Step 8/422\n",
      "Completed Step 9/422\n",
      "Completed Step 10/422\n",
      "Completed Step 11/422\n",
      "Completed Step 12/422\n",
      "Completed Step 13/422\n",
      "Completed Step 14/422\n",
      "Completed Step 15/422\n",
      "Completed Step 16/422\n",
      "Completed Step 17/422\n",
      "Completed Step 18/422\n",
      "Completed Step 19/422\n",
      "Completed Step 20/422\n",
      "Completed Step 21/422\n",
      "Completed Step 22/422\n",
      "Completed Step 23/422\n",
      "Completed Step 24/422\n",
      "Completed Step 25/422\n",
      "Completed Step 26/422\n",
      "Completed Step 27/422\n",
      "Completed Step 28/422\n",
      "Completed Step 29/422\n",
      "Completed Step 30/422\n",
      "Completed Step 31/422\n",
      "Completed Step 32/422\n",
      "Completed Step 33/422\n",
      "Completed Step 34/422\n",
      "Completed Step 35/422\n",
      "Completed Step 36/422\n",
      "Completed Step 37/422\n",
      "Completed Step 38/422\n",
      "Completed Step 39/422\n",
      "Completed Step 40/422\n",
      "Completed Step 41/422\n",
      "Completed Step 42/422\n",
      "Completed Step 43/422\n",
      "Completed Step 44/422\n",
      "Completed Step 45/422\n",
      "Completed Step 46/422\n",
      "Completed Step 47/422\n",
      "Completed Step 48/422\n",
      "Completed Step 49/422\n",
      "Completed Step 50/422\n",
      "Completed Step 51/422\n",
      "Completed Step 52/422\n",
      "Completed Step 53/422\n",
      "Completed Step 54/422\n",
      "Completed Step 55/422\n",
      "Completed Step 56/422\n",
      "Completed Step 57/422\n",
      "Completed Step 58/422\n",
      "Completed Step 59/422\n",
      "Completed Step 60/422\n",
      "Completed Step 61/422\n",
      "Completed Step 62/422\n",
      "Completed Step 63/422\n",
      "Completed Step 64/422\n",
      "Completed Step 65/422\n",
      "Completed Step 66/422\n",
      "Completed Step 67/422\n",
      "Completed Step 68/422\n",
      "Completed Step 69/422\n",
      "Completed Step 70/422\n",
      "Completed Step 71/422\n",
      "Completed Step 72/422\n",
      "Completed Step 73/422\n",
      "Completed Step 74/422\n",
      "Completed Step 75/422\n",
      "Completed Step 76/422\n",
      "Completed Step 77/422\n",
      "Completed Step 78/422\n",
      "Completed Step 79/422\n",
      "Completed Step 80/422\n",
      "Completed Step 81/422\n",
      "Completed Step 82/422\n",
      "Completed Step 83/422\n",
      "Completed Step 84/422\n",
      "Completed Step 85/422\n",
      "Completed Step 86/422\n",
      "Completed Step 87/422\n",
      "Completed Step 88/422\n",
      "Completed Step 89/422\n",
      "Completed Step 90/422\n",
      "Completed Step 91/422\n",
      "Completed Step 92/422\n",
      "Completed Step 93/422\n",
      "Completed Step 94/422\n",
      "Completed Step 95/422\n",
      "Completed Step 96/422\n",
      "Completed Step 97/422\n",
      "Completed Step 98/422\n",
      "Completed Step 99/422\n",
      "Completed Step 100/422\n",
      "Completed Step 101/422\n",
      "Completed Step 102/422\n",
      "Completed Step 103/422\n",
      "Completed Step 104/422\n",
      "Completed Step 105/422\n",
      "Completed Step 106/422\n",
      "Completed Step 107/422\n",
      "Completed Step 108/422\n",
      "Completed Step 109/422\n",
      "Completed Step 110/422\n",
      "Completed Step 111/422\n",
      "Completed Step 112/422\n",
      "Completed Step 113/422\n",
      "Completed Step 114/422\n",
      "Completed Step 115/422\n",
      "Completed Step 116/422\n",
      "Completed Step 117/422\n",
      "Completed Step 118/422\n",
      "Completed Step 119/422\n",
      "Completed Step 120/422\n",
      "Completed Step 121/422\n",
      "Completed Step 122/422\n",
      "Completed Step 123/422\n",
      "Completed Step 124/422\n",
      "Completed Step 125/422\n",
      "Completed Step 126/422\n",
      "Completed Step 127/422\n",
      "Completed Step 128/422\n",
      "Completed Step 129/422\n",
      "Completed Step 130/422\n",
      "Completed Step 131/422\n",
      "Completed Step 132/422\n",
      "Completed Step 133/422\n",
      "Completed Step 134/422\n",
      "Completed Step 135/422\n",
      "Completed Step 136/422\n",
      "Completed Step 137/422\n",
      "Completed Step 138/422\n",
      "Completed Step 139/422\n",
      "Completed Step 140/422\n",
      "Completed Step 141/422\n",
      "Completed Step 142/422\n",
      "Completed Step 143/422\n",
      "Completed Step 144/422\n",
      "Completed Step 145/422\n",
      "Completed Step 146/422\n",
      "Completed Step 147/422\n",
      "Completed Step 148/422\n",
      "Completed Step 149/422\n",
      "Completed Step 150/422\n",
      "Completed Step 151/422\n",
      "Completed Step 152/422\n",
      "Completed Step 153/422\n",
      "Completed Step 154/422\n",
      "Completed Step 155/422\n",
      "Completed Step 156/422\n",
      "Completed Step 157/422\n",
      "Completed Step 158/422\n",
      "Completed Step 159/422\n",
      "Completed Step 160/422\n",
      "Completed Step 161/422\n",
      "Completed Step 162/422\n",
      "Completed Step 163/422\n",
      "Completed Step 164/422\n",
      "Completed Step 165/422\n",
      "Completed Step 166/422\n",
      "Completed Step 167/422\n",
      "Completed Step 168/422\n",
      "Completed Step 169/422\n",
      "Completed Step 170/422\n",
      "Completed Step 171/422\n",
      "Completed Step 172/422\n",
      "Completed Step 173/422\n",
      "Completed Step 174/422\n",
      "Completed Step 175/422\n",
      "Completed Step 176/422\n",
      "Completed Step 177/422\n",
      "Completed Step 178/422\n",
      "Completed Step 179/422\n",
      "Completed Step 180/422\n",
      "Completed Step 181/422\n",
      "Completed Step 182/422\n",
      "Completed Step 183/422\n",
      "Completed Step 184/422\n",
      "Completed Step 185/422\n",
      "Completed Step 186/422\n",
      "Completed Step 187/422\n",
      "Completed Step 188/422\n",
      "Completed Step 189/422\n",
      "Completed Step 190/422\n",
      "Completed Step 191/422\n",
      "Completed Step 192/422\n",
      "Completed Step 193/422\n",
      "Completed Step 194/422\n",
      "Completed Step 195/422\n",
      "Completed Step 196/422\n",
      "Completed Step 197/422\n",
      "Completed Step 198/422\n",
      "Completed Step 199/422\n",
      "Completed Step 200/422\n",
      "Completed Step 201/422\n",
      "Completed Step 202/422\n",
      "Completed Step 203/422\n",
      "Completed Step 204/422\n",
      "Completed Step 205/422\n",
      "Completed Step 206/422\n",
      "Completed Step 207/422\n",
      "Completed Step 208/422\n",
      "Completed Step 209/422\n",
      "Completed Step 210/422\n",
      "Completed Step 211/422\n",
      "Completed Step 212/422\n",
      "Completed Step 213/422\n",
      "Completed Step 214/422\n",
      "Completed Step 215/422\n",
      "Completed Step 216/422\n",
      "Completed Step 217/422\n",
      "Completed Step 218/422\n",
      "Completed Step 219/422\n",
      "Completed Step 220/422\n",
      "Completed Step 221/422\n",
      "Completed Step 222/422\n",
      "Completed Step 223/422\n",
      "Completed Step 224/422\n",
      "Completed Step 225/422\n",
      "Completed Step 226/422\n",
      "Completed Step 227/422\n",
      "Completed Step 228/422\n",
      "Completed Step 229/422\n",
      "Completed Step 230/422\n",
      "Completed Step 231/422\n",
      "Completed Step 232/422\n",
      "Completed Step 233/422\n",
      "Completed Step 234/422\n",
      "Completed Step 235/422\n",
      "Completed Step 236/422\n",
      "Completed Step 237/422\n",
      "Completed Step 238/422\n",
      "Completed Step 239/422\n",
      "Completed Step 240/422\n",
      "Completed Step 241/422\n",
      "Completed Step 242/422\n",
      "Completed Step 243/422\n",
      "Completed Step 244/422\n",
      "Completed Step 245/422\n",
      "Completed Step 246/422\n",
      "Completed Step 247/422\n",
      "Completed Step 248/422\n",
      "Completed Step 249/422\n",
      "Completed Step 250/422\n",
      "Completed Step 251/422\n",
      "Completed Step 252/422\n",
      "Completed Step 253/422\n",
      "Completed Step 254/422\n",
      "Completed Step 255/422\n",
      "Completed Step 256/422\n",
      "Completed Step 257/422\n",
      "Completed Step 258/422\n",
      "Completed Step 259/422\n",
      "Completed Step 260/422\n",
      "Completed Step 261/422\n",
      "Completed Step 262/422\n",
      "Completed Step 263/422\n",
      "Completed Step 264/422\n",
      "Completed Step 265/422\n",
      "Completed Step 266/422\n",
      "Completed Step 267/422\n",
      "Completed Step 268/422\n",
      "Completed Step 269/422\n",
      "Completed Step 270/422\n",
      "Completed Step 271/422\n",
      "Completed Step 272/422\n",
      "Completed Step 273/422\n",
      "Completed Step 274/422\n",
      "Completed Step 275/422\n",
      "Completed Step 276/422\n",
      "Completed Step 277/422\n",
      "Completed Step 278/422\n",
      "Completed Step 279/422\n",
      "Completed Step 280/422\n",
      "Completed Step 281/422\n",
      "Completed Step 282/422\n",
      "Completed Step 283/422\n",
      "Completed Step 284/422\n",
      "Completed Step 285/422\n",
      "Completed Step 286/422\n",
      "Completed Step 287/422\n",
      "Completed Step 288/422\n",
      "Completed Step 289/422\n",
      "Completed Step 290/422\n",
      "Completed Step 291/422\n",
      "Completed Step 292/422\n",
      "Completed Step 293/422\n",
      "Completed Step 294/422\n",
      "Completed Step 295/422\n",
      "Completed Step 296/422\n",
      "Completed Step 297/422\n",
      "Completed Step 298/422\n",
      "Completed Step 299/422\n",
      "Completed Step 300/422\n",
      "Completed Step 301/422\n",
      "Completed Step 302/422\n",
      "Completed Step 303/422\n",
      "Completed Step 304/422\n",
      "Completed Step 305/422\n",
      "Completed Step 306/422\n",
      "Completed Step 307/422\n",
      "Completed Step 308/422\n",
      "Completed Step 309/422\n",
      "Completed Step 310/422\n",
      "Completed Step 311/422\n",
      "Completed Step 312/422\n",
      "Completed Step 313/422\n",
      "Completed Step 314/422\n",
      "Completed Step 315/422\n",
      "Completed Step 316/422\n",
      "Completed Step 317/422\n",
      "Completed Step 318/422\n",
      "Completed Step 319/422\n",
      "Completed Step 320/422\n",
      "Completed Step 321/422\n",
      "Completed Step 322/422\n",
      "Completed Step 323/422\n",
      "Completed Step 324/422\n",
      "Completed Step 325/422\n",
      "Completed Step 326/422\n",
      "Completed Step 327/422\n",
      "Completed Step 328/422\n",
      "Completed Step 329/422\n",
      "Completed Step 330/422\n",
      "Completed Step 331/422\n",
      "Completed Step 332/422\n",
      "Completed Step 333/422\n",
      "Completed Step 334/422\n",
      "Completed Step 335/422\n",
      "Completed Step 336/422\n",
      "Completed Step 337/422\n",
      "Completed Step 338/422\n",
      "Completed Step 339/422\n",
      "Completed Step 340/422\n",
      "Completed Step 341/422\n",
      "Completed Step 342/422\n",
      "Completed Step 343/422\n",
      "Completed Step 344/422\n",
      "Completed Step 345/422\n",
      "Completed Step 346/422\n",
      "Completed Step 347/422\n",
      "Completed Step 348/422\n",
      "Completed Step 349/422\n",
      "Completed Step 350/422\n",
      "Completed Step 351/422\n",
      "Completed Step 352/422\n",
      "Completed Step 353/422\n",
      "Completed Step 354/422\n",
      "Completed Step 355/422\n",
      "Completed Step 356/422\n",
      "Completed Step 357/422\n",
      "Completed Step 358/422\n",
      "Completed Step 359/422\n",
      "Completed Step 360/422\n",
      "Completed Step 361/422\n",
      "Completed Step 362/422\n",
      "Completed Step 363/422\n",
      "Completed Step 364/422\n",
      "Completed Step 365/422\n",
      "Completed Step 366/422\n",
      "Completed Step 367/422\n",
      "Completed Step 368/422\n",
      "Completed Step 369/422\n",
      "Completed Step 370/422\n",
      "Completed Step 371/422\n",
      "Completed Step 372/422\n",
      "Completed Step 373/422\n",
      "Completed Step 374/422\n",
      "Completed Step 375/422\n",
      "Completed Step 376/422\n",
      "Completed Step 377/422\n",
      "Completed Step 378/422\n",
      "Completed Step 379/422\n",
      "Completed Step 380/422\n",
      "Completed Step 381/422\n",
      "Completed Step 382/422\n",
      "Completed Step 383/422\n",
      "Completed Step 384/422\n",
      "Completed Step 385/422\n",
      "Completed Step 386/422\n",
      "Completed Step 387/422\n",
      "Completed Step 388/422\n",
      "Completed Step 389/422\n",
      "Completed Step 390/422\n",
      "Completed Step 391/422\n",
      "Completed Step 392/422\n",
      "Completed Step 393/422\n",
      "Completed Step 394/422\n",
      "Completed Step 395/422\n",
      "Completed Step 396/422\n",
      "Completed Step 397/422\n",
      "Completed Step 398/422\n",
      "Completed Step 399/422\n",
      "Completed Step 400/422\n",
      "Completed Step 401/422\n",
      "Completed Step 402/422\n",
      "Completed Step 403/422\n",
      "Completed Step 404/422\n",
      "Completed Step 405/422\n",
      "Completed Step 406/422\n",
      "Completed Step 407/422\n",
      "Completed Step 408/422\n",
      "Completed Step 409/422\n",
      "Completed Step 410/422\n",
      "Completed Step 411/422\n",
      "Completed Step 412/422\n",
      "Completed Step 413/422\n",
      "Completed Step 414/422\n",
      "Completed Step 415/422\n",
      "Completed Step 416/422\n",
      "Completed Step 417/422\n",
      "Completed Step 418/422\n",
      "Completed Step 419/422\n",
      "Completed Step 420/422\n",
      "Completed Step 421/422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/74v_v08n4wxftghd_jwkbqt40000gn/T/ipykernel_27007/1656397134.py:52: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Step 0/313\n",
      "Completed Step 1/313\n",
      "Completed Step 2/313\n",
      "Completed Step 3/313\n",
      "Completed Step 4/313\n",
      "Completed Step 5/313\n",
      "Completed Step 6/313\n",
      "Completed Step 7/313\n",
      "Completed Step 8/313\n",
      "Completed Step 9/313\n",
      "Completed Step 10/313\n",
      "Completed Step 11/313\n",
      "Completed Step 12/313\n",
      "Completed Step 13/313\n",
      "Completed Step 14/313\n",
      "Completed Step 15/313\n",
      "Completed Step 16/313\n",
      "Completed Step 17/313\n",
      "Completed Step 18/313\n",
      "Completed Step 19/313\n",
      "Completed Step 20/313\n",
      "Completed Step 21/313\n",
      "Completed Step 22/313\n",
      "Completed Step 23/313\n",
      "Completed Step 24/313\n",
      "Completed Step 25/313\n",
      "Completed Step 26/313\n",
      "Completed Step 27/313\n",
      "Completed Step 28/313\n",
      "Completed Step 29/313\n",
      "Completed Step 30/313\n",
      "Completed Step 31/313\n",
      "Completed Step 32/313\n",
      "Completed Step 33/313\n",
      "Completed Step 34/313\n",
      "Completed Step 35/313\n",
      "Completed Step 36/313\n",
      "Completed Step 37/313\n",
      "Completed Step 38/313\n",
      "Completed Step 39/313\n",
      "Completed Step 40/313\n",
      "Completed Step 41/313\n",
      "Completed Step 42/313\n",
      "Completed Step 43/313\n",
      "Completed Step 44/313\n",
      "Completed Step 45/313\n",
      "Completed Step 46/313\n",
      "Completed Step 47/313\n",
      "Completed Step 48/313\n",
      "Completed Step 49/313\n",
      "Completed Step 50/313\n",
      "Completed Step 51/313\n",
      "Completed Step 52/313\n",
      "Completed Step 53/313\n",
      "Completed Step 54/313\n",
      "Completed Step 55/313\n",
      "Completed Step 56/313\n",
      "Completed Step 57/313\n",
      "Completed Step 58/313\n",
      "Completed Step 59/313\n",
      "Completed Step 60/313\n",
      "Completed Step 61/313\n",
      "Completed Step 62/313\n",
      "Completed Step 63/313\n",
      "Completed Step 64/313\n",
      "Completed Step 65/313\n",
      "Completed Step 66/313\n",
      "Completed Step 67/313\n",
      "Completed Step 68/313\n",
      "Completed Step 69/313\n",
      "Completed Step 70/313\n",
      "Completed Step 71/313\n",
      "Completed Step 72/313\n",
      "Completed Step 73/313\n",
      "Completed Step 74/313\n",
      "Completed Step 75/313\n",
      "Completed Step 76/313\n",
      "Completed Step 77/313\n",
      "Completed Step 78/313\n",
      "Completed Step 79/313\n",
      "Completed Step 80/313\n",
      "Completed Step 81/313\n",
      "Completed Step 82/313\n",
      "Completed Step 83/313\n",
      "Completed Step 84/313\n",
      "Completed Step 85/313\n",
      "Completed Step 86/313\n",
      "Completed Step 87/313\n",
      "Completed Step 88/313\n",
      "Completed Step 89/313\n",
      "Completed Step 90/313\n",
      "Completed Step 91/313\n",
      "Completed Step 92/313\n",
      "Completed Step 93/313\n",
      "Completed Step 94/313\n",
      "Completed Step 95/313\n",
      "Completed Step 96/313\n",
      "Completed Step 97/313\n",
      "Completed Step 98/313\n",
      "Completed Step 99/313\n",
      "Completed Step 100/313\n",
      "Completed Step 101/313\n",
      "Completed Step 102/313\n",
      "Completed Step 103/313\n",
      "Completed Step 104/313\n",
      "Completed Step 105/313\n",
      "Completed Step 106/313\n",
      "Completed Step 107/313\n",
      "Completed Step 108/313\n",
      "Completed Step 109/313\n",
      "Completed Step 110/313\n",
      "Completed Step 111/313\n",
      "Completed Step 112/313\n",
      "Completed Step 113/313\n",
      "Completed Step 114/313\n",
      "Completed Step 115/313\n",
      "Completed Step 116/313\n",
      "Completed Step 117/313\n",
      "Completed Step 118/313\n",
      "Completed Step 119/313\n",
      "Completed Step 120/313\n",
      "Completed Step 121/313\n",
      "Completed Step 122/313\n",
      "Completed Step 123/313\n",
      "Completed Step 124/313\n",
      "Completed Step 125/313\n",
      "Completed Step 126/313\n",
      "Completed Step 127/313\n",
      "Completed Step 128/313\n",
      "Completed Step 129/313\n",
      "Completed Step 130/313\n",
      "Completed Step 131/313\n",
      "Completed Step 132/313\n",
      "Completed Step 133/313\n",
      "Completed Step 134/313\n",
      "Completed Step 135/313\n",
      "Completed Step 136/313\n",
      "Completed Step 137/313\n",
      "Completed Step 138/313\n",
      "Completed Step 139/313\n",
      "Completed Step 140/313\n",
      "Completed Step 141/313\n",
      "Completed Step 142/313\n",
      "Completed Step 143/313\n",
      "Completed Step 144/313\n",
      "Completed Step 145/313\n",
      "Completed Step 146/313\n",
      "Completed Step 147/313\n",
      "Completed Step 148/313\n",
      "Completed Step 149/313\n",
      "Completed Step 150/313\n",
      "Completed Step 151/313\n",
      "Completed Step 152/313\n",
      "Completed Step 153/313\n",
      "Completed Step 154/313\n",
      "Completed Step 155/313\n",
      "Completed Step 156/313\n",
      "Completed Step 157/313\n",
      "Completed Step 158/313\n",
      "Completed Step 159/313\n",
      "Completed Step 160/313\n",
      "Completed Step 161/313\n",
      "Completed Step 162/313\n",
      "Completed Step 163/313\n",
      "Completed Step 164/313\n",
      "Completed Step 165/313\n",
      "Completed Step 166/313\n",
      "Completed Step 167/313\n",
      "Completed Step 168/313\n",
      "Completed Step 169/313\n",
      "Completed Step 170/313\n",
      "Completed Step 171/313\n",
      "Completed Step 172/313\n",
      "Completed Step 173/313\n",
      "Completed Step 174/313\n",
      "Completed Step 175/313\n",
      "Completed Step 176/313\n",
      "Completed Step 177/313\n",
      "Completed Step 178/313\n",
      "Completed Step 179/313\n",
      "Completed Step 180/313\n",
      "Completed Step 181/313\n",
      "Completed Step 182/313\n",
      "Completed Step 183/313\n",
      "Completed Step 184/313\n",
      "Completed Step 185/313\n",
      "Completed Step 186/313\n",
      "Completed Step 187/313\n",
      "Completed Step 188/313\n",
      "Completed Step 189/313\n",
      "Completed Step 190/313\n",
      "Completed Step 191/313\n",
      "Completed Step 192/313\n",
      "Completed Step 193/313\n",
      "Completed Step 194/313\n",
      "Completed Step 195/313\n",
      "Completed Step 196/313\n",
      "Completed Step 197/313\n",
      "Completed Step 198/313\n",
      "Completed Step 199/313\n",
      "Completed Step 200/313\n",
      "Completed Step 201/313\n",
      "Completed Step 202/313\n",
      "Completed Step 203/313\n",
      "Completed Step 204/313\n",
      "Completed Step 205/313\n",
      "Completed Step 206/313\n",
      "Completed Step 207/313\n",
      "Completed Step 208/313\n",
      "Completed Step 209/313\n",
      "Completed Step 210/313\n",
      "Completed Step 211/313\n",
      "Completed Step 212/313\n",
      "Completed Step 213/313\n",
      "Completed Step 214/313\n",
      "Completed Step 215/313\n",
      "Completed Step 216/313\n",
      "Completed Step 217/313\n",
      "Completed Step 218/313\n",
      "Completed Step 219/313\n",
      "Completed Step 220/313\n",
      "Completed Step 221/313\n",
      "Completed Step 222/313\n",
      "Completed Step 223/313\n",
      "Completed Step 224/313\n",
      "Completed Step 225/313\n",
      "Completed Step 226/313\n",
      "Completed Step 227/313\n",
      "Completed Step 228/313\n",
      "Completed Step 229/313\n",
      "Completed Step 230/313\n",
      "Completed Step 231/313\n",
      "Completed Step 232/313\n",
      "Completed Step 233/313\n",
      "Completed Step 234/313\n",
      "Completed Step 235/313\n",
      "Completed Step 236/313\n",
      "Completed Step 237/313\n",
      "Completed Step 238/313\n",
      "Completed Step 239/313\n",
      "Completed Step 240/313\n",
      "Completed Step 241/313\n",
      "Completed Step 242/313\n",
      "Completed Step 243/313\n",
      "Completed Step 244/313\n",
      "Completed Step 245/313\n",
      "Completed Step 246/313\n",
      "Completed Step 247/313\n",
      "Completed Step 248/313\n",
      "Completed Step 249/313\n",
      "Completed Step 250/313\n",
      "Completed Step 251/313\n",
      "Completed Step 252/313\n",
      "Completed Step 253/313\n",
      "Completed Step 254/313\n",
      "Completed Step 255/313\n",
      "Completed Step 256/313\n",
      "Completed Step 257/313\n",
      "Completed Step 258/313\n",
      "Completed Step 259/313\n",
      "Completed Step 260/313\n",
      "Completed Step 261/313\n",
      "Completed Step 262/313\n",
      "Completed Step 263/313\n",
      "Completed Step 264/313\n",
      "Completed Step 265/313\n",
      "Completed Step 266/313\n",
      "Completed Step 267/313\n",
      "Completed Step 268/313\n",
      "Completed Step 269/313\n",
      "Completed Step 270/313\n",
      "Completed Step 271/313\n",
      "Completed Step 272/313\n",
      "Completed Step 273/313\n",
      "Completed Step 274/313\n",
      "Completed Step 275/313\n",
      "Completed Step 276/313\n",
      "Completed Step 277/313\n",
      "Completed Step 278/313\n",
      "Completed Step 279/313\n",
      "Completed Step 280/313\n",
      "Completed Step 281/313\n",
      "Completed Step 282/313\n",
      "Completed Step 283/313\n",
      "Completed Step 284/313\n",
      "Completed Step 285/313\n",
      "Completed Step 286/313\n",
      "Completed Step 287/313\n",
      "Completed Step 288/313\n",
      "Completed Step 289/313\n",
      "Completed Step 290/313\n",
      "Completed Step 291/313\n",
      "Completed Step 292/313\n",
      "Completed Step 293/313\n",
      "Completed Step 294/313\n",
      "Completed Step 295/313\n",
      "Completed Step 296/313\n",
      "Completed Step 297/313\n",
      "Completed Step 298/313\n",
      "Completed Step 299/313\n",
      "Completed Step 300/313\n",
      "Completed Step 301/313\n",
      "Completed Step 302/313\n",
      "Completed Step 303/313\n",
      "Completed Step 304/313\n",
      "Completed Step 305/313\n",
      "Completed Step 306/313\n",
      "Completed Step 307/313\n",
      "Completed Step 308/313\n",
      "Completed Step 309/313\n",
      "Completed Step 310/313\n",
      "Completed Step 311/313\n",
      "Completed Step 312/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/74v_v08n4wxftghd_jwkbqt40000gn/T/ipykernel_27007/1656397134.py:52: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "import imageio\n",
    "\n",
    "# Visualising the episodes.\n",
    "def visualize_episode_path(maze_env, episode, upto_timestep, filename):\n",
    "    n, m = maze_env._shape\n",
    "    grid = np.zeros(maze_env._shape)\n",
    "\n",
    "    # Extract past states up to the current timestep\n",
    "    past_states = [maze_env._get_loc_from_state(t[1]) for t in episode[:upto_timestep+1]]\n",
    "\n",
    "    # Set alpha decrement based on the number of states to visualize\n",
    "    decrement = 0.5 / len(past_states)\n",
    "    alpha = 1.0\n",
    "\n",
    "    # Fill in the grid based on the past states\n",
    "    for (agent_i, agent_j) in reversed(past_states):\n",
    "        if grid[agent_i][agent_j] < alpha:  # Only update if the current opacity is less than the new one\n",
    "            grid[agent_i][agent_j] = alpha\n",
    "        alpha -= decrement\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(m, n))\n",
    "    cax = ax.matshow(grid, cmap='viridis')  # Use a colormap to visualize the path\n",
    "\n",
    "    # Mark obstacles and absorbing states\n",
    "    for i, j in maze_env._obstacle_locs:\n",
    "        ax.text(j, i, '█', ha='center', va='center', fontsize=12, color='white')\n",
    "    for i, j in maze_env._absorbing_locs:\n",
    "        ax.text(j, i, 'A', ha='center', va='center', fontsize=12, color='white')\n",
    "\n",
    "    ax.set_xticks(np.arange(-0.5, m, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, n, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(which='both')\n",
    "\n",
    "    plt.savefig(filename, dpi=30)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_gif(maze, episode, fs):\n",
    "    image_filenames = []\n",
    "    episode_length = len(episode)\n",
    "    for idx in range(episode_length):\n",
    "        print(f\"Completed Step {idx}/{episode_length}\")\n",
    "        filename = f\"./images/MC/Episodes/path_upto_timestep_{idx}.png\"\n",
    "        visualize_episode_path(maze, episode, idx, filename)\n",
    "        image_filenames.append(filename)\n",
    "\n",
    "    with imageio.get_writer(fs, mode='I', duration=0.5) as writer:  # 0.5 seconds per frame\n",
    "        for filename in image_filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "            os.remove(filename)\n",
    "\n",
    "generate_gif(maze, batches[1000][0], \"./images/MC/episode_1000.gif\")\n",
    "generate_gif(maze, batches[2000][0], \"./images/MC/episode_2000.gif\")\n",
    "generate_gif(maze, batches[3000][0], \"./images/MC/episode_3000.gif\")\n",
    "generate_gif(maze, batches[4000][0], \"./images/MC/episode_4000.gif\")\n",
    "generate_gif(maze, batches[5000][0], \"./images/MC/episode_5000.gif\")\n",
    "generate_gif(maze, batches[6000][0], \"./images/MC/episode_6000.gif\")\n",
    "generate_gif(maze, batches[7000][0], \"./images/MC/episode_7000.gif\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMkZKrh6VUgw"
   },
   "source": [
    "## TD agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Xyko9SvrGbE"
   },
   "outputs": [],
   "source": [
    "# This class define the Temporal-Difference agent\n",
    "\n",
    "class TD_agent(object):\n",
    "\n",
    "  # [Action required]\n",
    "  # WARNING: make sure this function can be called by the auto-marking script\n",
    "  def solve(self, env):\n",
    "    \"\"\"\n",
    "    Solve a given Maze environment using Temporal Difference learning\n",
    "    input: env {Maze object} -- Maze to solve\n",
    "    output: \n",
    "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
    "      - values {list of np.array} -- List of successive value functions for each episode \n",
    "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation (can be edited)\n",
    "    Q = np.random.rand(env.get_state_size(), env.get_action_size()) \n",
    "    V = np.zeros(env.get_state_size())\n",
    "    policy = np.zeros((env.get_state_size(), env.get_action_size())) \n",
    "    values = [V]\n",
    "    total_rewards = []\n",
    "\n",
    "    #### \n",
    "    # Add your code here\n",
    "    # WARNING: this agent only has access to env.reset() and env.step()\n",
    "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
    "    ####\n",
    "    \n",
    "    return policy, values, total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzSzRSO6VWVD"
   },
   "source": [
    "## Example main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eyeJfvwXp3ta",
    "outputId": "229af227-7973-4819-dc05-ff8219987805"
   },
   "outputs": [],
   "source": [
    "# Example main (can be edited)\n",
    "\n",
    "### Question 0: Defining the environment\n",
    "\n",
    "print(\"Creating the Maze:\\n\")\n",
    "maze = Maze()\n",
    "\n",
    "\n",
    "### Question 1: Dynamic programming\n",
    "\n",
    "dp_agent = DP_agent()\n",
    "dp_policy, dp_value = dp_agent.solve(maze)\n",
    "\n",
    "print(\"Results of the DP agent:\\n\")\n",
    "maze.get_graphics().draw_policy(dp_policy)\n",
    "maze.get_graphics().draw_value(dp_value)\n",
    "\n",
    "\n",
    "### Question 2: Monte-Carlo learning\n",
    "\n",
    "mc_agent = MC_agent()\n",
    "mc_policy, mc_values, total_rewards = mc_agent.solve(maze)\n",
    "\n",
    "print(\"Results of the MC agent:\\n\")\n",
    "maze.get_graphics().draw_policy(mc_policy)\n",
    "maze.get_graphics().draw_value(mc_values[-1])\n",
    "\n",
    "\n",
    "### Question 3: Temporal-Difference learning\n",
    "\n",
    "td_agent = TD_agent()\n",
    "td_policy, td_values, total_rewards = td_agent.solve(maze)\n",
    "\n",
    "print(\"Results of the TD agent:\\n\")\n",
    "maze.get_graphics().draw_policy(td_policy)\n",
    "maze.get_graphics().draw_value(td_values[-1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lbY8DCqoVJlw",
    "DW3Ul0q-VRE-",
    "14i0zRkdVSqk"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
